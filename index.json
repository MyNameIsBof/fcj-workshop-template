[{"uri":"https://mynameisbof.github.io/fcj-workshop-template/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Orchestrating document processing with AWS AppSync Events and Amazon Bedrock by Mehdi Amrane on July 9, 2025 in Advanced (300), Amazon Bedrock, Amazon Cognito, AWS Amplify, AWS AppSync, AWS Lambda, AWS Step Functions, Serverless, Technical How-to Permalink Share\nMany organizations implement intelligent document processing workflows to extract meaningful insights from an ever-increasing volume of unstructured content (such as insurance claims, loan applications, etc.). Traditionally, these processes require significant technical effort, as implementations often involve using multiple machine learning (ML) models and orchestrating complex workflows.\nWhen organizations integrate these channels into customer-facing applications (such as web applications for customers to upload documents like insurance claims, loan approval documents, etc.), they aim to provide real-time insights to enhance end-user experience. These organizations also aim to operate and scale these workloads with minimal operational overhead and cost optimization. Additionally, these organizations need to implement common security measures such as identity and access management to ensure that only authenticated and authorized users are allowed to perform specific actions or access specific resources.\nIn this article, we introduce a solution that simplifies the creation of intelligent document processing workflows, with a web application that allows customers to upload files (documents and images) and gather insights from them (summarization, field extraction, and classification). This solution primarily uses serverless technology, including a websocket to receive real-time insights and provides many benefits, such as automatic scaling, built-in high availability, and a pay-per-use pricing model for cost optimization. The solution also includes an authentication layer and an authorization layer to manage identity and permissions.\nSolution overview In this post, we provide an overview of how the solution works and then describe how to set up the solution with the following services:\nAmazon Bedrock and Amazon Bedrock Data Automation to summarize the content of uploaded files (documents or images) and generate insights from them. AWS Step Functions and AWS Lambda to orchestrate summarization and extraction operations, using Amazon Bedrock and Amazon Bedrock Data Automation. AWS AppSync Events to create a serverless websocket for the web application to receive summarization and extraction insights in real time. AWS Amplify to create and deploy the web application Amazon EventBridge to trigger the orchestration workflow (using AWS Step Functions and AWS Lambda) when a new file is uploaded Amazon Cognito to implement an identity platform (user directory management and authorization) for the web application. Amazon Simple Storage Service (Amazon S3) to store uploaded files (for processing by the processing pipeline) and assets related to the web application. The solution architecture is illustrated in the following diagram:\nStep 1: User authenticates with the web application (hosted on AWS Amplify).\nStep 2: Amazon Cognito authenticates credentials. Then, the user is logged into the web application.\nStep 3a and 3b:\nStep 3a: Web application (AWS Amplify) subscribes to AWS AppSync Events websocket. Step 3b: AWS AppSync Events websocket calls AWS Lambda authorizer to verify that the user is allowed to subscribe to the web socket. Step 4: User uploads a file (document or image) through the web application.\nStep 5: Web application (hosted on AWS Amplify) calls Amazon Cognito (identity pool) to verify that the user is allowed to upload files.\nStep 6: File is uploaded to Amazon S3 bucket.\nStep 7a and 7b: When receiving an Amazon S3 upload event (notifying that a file has been uploaded to the Amazon S3 bucket) in the default Amazon EventBridge bus, an Amazon EventBridge bus rule triggers AWS Step Functions state machine execution to start the orchestration workflow.\nStep 8 (Extract fields from file and classify file):\nStep 8a: First AWS Lambda function launches a new Amazon Bedrock Automation task (this task extracts specific fields from the uploaded file and classifies it). Step 8b: After the task completes, results are stored in an Amazon S3 bucket. Step 8c and 8d: When receiving an Amazon S3 event (notifying that results have been stored in the Amazon S3 bucket) on the default Amazon EventBridge, an Amazon EventBridge bus rule triggers execution of an AWS Lambda function. Step 8e: An AWS Lambda function publishes results to the web socket. Step 9a and 9b: Second AWS Lambda function sends a prompt to Amazon Bedrock foundation model (Sonnet 3) to request summarization in the stream of the uploaded file. AWS Lambda function publishes streaming data to the web socket.\nAfter Step 8e and Step 9b, the user can now view summarization results and file extraction insights for the uploaded file in the web application.\nPrerequisites To implement and set up this solution, you must have the following:\nAWS account Device with access to your AWS account with the following: Python 3.12 installed (including pip) Node.js 20.12.0 installed Model Access enabled to Claude 3 Sonnet model in Amazon Bedrock Note: Deploying this solution will incur costs. Please review the pricing page of each AWS service used in this article for cost details. The operating costs of this solution primarily depend on:\nNumber of documents (and size of each document) Number of active users Setting up Amazon Bedrock Data Automation In this section, we set up an Amazon Bedrock Data Automation project and Amazon Bedrock blueprint.\nA project includes a list of blueprints, and each blueprint defines fields to extract from different file types (such as documents or images). In this article, we will define a blueprint for driver\u0026rsquo;s licenses.\nComplete the following steps to create an Amazon Bedrock Data Automation project and a driver\u0026rsquo;s license blueprint:\nClone the GitHub repository:\ngit clone https://github.com/aws-samples/sample-create-idp-with-appsyncevents-and-amazonbedrock.git Navigate to the sample-create-idp-with-appsyncevents-and-amazonbedrock folder:\ncd sample-create-idp-with-appsyncevents-and-amazonbedrock Initialize the environment (prepare shell script files from the GitHub repository for use):\nchmod +x ./init-env.sh \u0026amp;\u0026amp; source ./init-env.sh Run the setup-bda-project.sh script to create an Amazon Bedrock Data Automation project and sample driver\u0026rsquo;s license blueprint:\n./setup-bda-project.sh Creating websocket and orchestration backend In this section, we create the following resources:\nA user directory for web authentication and authorization, created using Amazon Cognito user pool. An Amazon Cognito identity pool is also created to authenticate that users are allowed to upload files through the web application. A websocket using AWS AppSync Events. This allows our web application to receive real-time updates for summarization and extraction results. An authorization layer is also created to protect the websocket from unauthorized users. This layer is implemented with a Lambda authorization function to verify that incoming requests include valid authorization information. A state machine using AWS Step Functions and AWS Lambda to orchestrate summarization and extraction operations from unstructured content Amazon S3 to store and encrypt AWS Lambda functions. Complete the following steps to create the websocket and orchestration backend of the solution, using AWS CloudFormation:\nCreate the Amazon S3 buckets used by the solution by running the following script. These buckets will store files uploaded by users and code files of AWS Lambda functions used in this solution.\ncd $CURRENT_DIR/s3; ./create-s3-buckets.sh Create Amazon Cognito user pool and identity pool by running the create-cognito-userpool.sh script:\ncd $CURRENT_DIR/cognito; ./create-cognito-userpool.sh Create AWS AppSync Events web socket by running the following script:\ncd $CURRENT_DIR/appsync/; ./create-appsync-api.sh Create AWS Step Functions state machine (including AWS Lambda functions) by running the following scripts:\ncd $CURRENT_DIR/orchestration/; ./create-orchestration.sh Configuring Amazon Cognito user pool In this section, we will create a user in the Amazon Cognito user pool. This user will log into our web application.\nRun the create-cognito-testuser.sh script command to create a user (make sure to provide your email address):\ncd $CURRENT_DIR/cognito; ./create-cognito-testuser.sh #your-email-address# After you create the user, you will receive an email with a temporary password in this format: \u0026ldquo;Your username is #your-email-address# and temporary password is #temporary-password#.\u0026rdquo;\nPlease record these login credentials (email address and temporary password) for later use when testing the web application.\nCreating the web application In this section, we build a web application using AWS Amplify and publish the application so it can be accessed through an endpoint URL.\nComplete the following steps to create the web application:\nRun the create-webapp.sh script to create the web application with AWS Amplify:\ncd $CURRENT_DIR/amplify/; ./create-webapp.sh Run the deploy.sh script to deploy the web application:\ncd $CURRENT_DIR/amplify/amplify-idp; ./deploy.sh The web application is now available for testing and a URL will be displayed, as shown in the screenshot below. Please note the URL for use in the next section.\nTesting the web application In this section, we will test the web application and upload a file for processing:\nOpen the AWS Amplify application URL in your web browser. Enter login credentials (email and temporary password you received earlier when configuring the user pool in Amazon Cognito) and select Sign in. When prompted, enter a new password and select Change password. You can now see the web interface. Download a sample driver\u0026rsquo;s license at this location and upload it through the web application using your camera or a file from your local device, as illustrated. After the file is uploaded, you will start receiving feedback in the web application. When all operations are complete, you will see results equivalent to the following screenshot: Note: If you intend to use other sample driver\u0026rsquo;s license images with different formats, you may need to update the existing Bedrock Data Automation blueprint we created earlier or define a new blueprint in the Bedrock Data Automation project we created earlier for these new images to work. For more information, please see Bedrock Data Automation documentation.\nCleanup To ensure no additional costs are incurred, delete the resources that have been provisioned in your account. Make sure you are using the correct AWS account before deleting the following resources.\nImportant Note: You should be careful when performing the steps above. Make sure you are deleting resources in the correct AWS account.\nYou can navigate to the AWS CloudFormation console to delete CloudFormation stacks associated with the provisioned resources or use the cleanup helper script cleanup.sh available at the root of the sample-create-idp-with-appsyncevents-and-amazonbedrock directory:\n./cleanup.sh #region# Conclusion In this article, we introduced a solution for creating document processing workflows, with a web application using serverless services. Through the web application, we can upload files and receive real-time feedback for different types of operations (summarization, extraction of specific fields, and classification). First, we created an Amazon Bedrock Data Automation project (with a driver\u0026rsquo;s license blueprint). Then, we created a web socket along with an orchestration solution using a state machine (AWS Step Functions and AWS Lambda functions). We also configured a user pool to grant users access to the web application. Finally, we created the user interface (frontend) of the web application on AWS Amplify.\nTo dive deeper into this solution, a self-paced workshop is available in AWS Workshop Studio.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"How federal agency leaders are using AWS tools to improve efficiency By Emma Harrison on July 9, 2025 in Amazon SageMaker, Artificial Intelligence, Customer Solutions, Generative AI, Government, Migration, Public Sector Permalink Share\nFederal agencies are working to meet the demand for systems that can keep up with a tech-savvy public. Agency leaders are seizing the opportunity to restructure government by modernizing legacy systems and finding safer ways to serve citizens digitally. At the AWS Summit in Washington, DC 2025, federal leaders presented how they are using Amazon Web Services (AWS) to improve efficiency.\nAgencies are using cloud tools to increase productivity, minimize resource usage, and improve their ability to serve the public effectively. This article highlights pioneering leaders who are opening new paths in government cloud computing.\nFCC modernizes 94 percent of systems The Federal Communications Commission (FCC) was struggling with outdated, legacy infrastructure that was expensive and inefficient to maintain. Frequent hardware challenges, outdated application technology stacks, and increasing cybersecurity compliance challenges were putting pressure on the agency. High operating costs, limited scalability, and dependence on specialized skill sets to manage legacy systems were slowing progress. Don Tweedie, FCC Deputy Chief Information Officer for Technology Delivery, and his team recognized the need for a fundamental transformation.\nTweedie led this effort by building a strategic playbook to modernize the FCC\u0026rsquo;s network architecture. His vision focused on a cloud-first approach—prioritizing scalability, security, and flexibility. The agency began treating data as a strategic asset, implementing micro-segmentation, and leveraging native cloud features like auto-scaling and serverless computing. The result was a shift toward data-driven decision-making and a more flexible infrastructure.\nIn just 20 months, the FCC moved 94 percent of its data center operations to the cloud and decommissioned approximately 300 underutilized servers—streamlining the technology stack and reducing operational stagnation.\nTweedie emphasized the importance of starting with small, manageable pilot programs to build momentum and organizational buy-in. He recommended engaging security groups early, reinforcing zero-trust principles, upskilling current employees to address talent shortages, and establishing clear, actionable policies to guide the transformation.\nKCNSC bridges collaboration gap The Kansas City National Security Campus (KCNSC) operates on behalf of the National Nuclear Security Administration (NNSA) under the Department of Energy to protect the national nuclear program. With a distributed workforce of more than 7,000 people, the agency was increasingly struggling to build an effective collaborative environment in the unclassified space. The KCNSC team was grappling with inefficient data distribution, lengthy discussions about agency resource usage, and scaling challenges.\nTo address this urgent issue, Victor Doane, Enterprise Architect for Digital Transformation Program, pioneered the idea of a Joint Unclassified Cloud Environment (JUCE). He invited technical leaders and other groups to come together to identify organizational needs and discuss their different cloud transformation journeys to learn from each other\u0026rsquo;s experiences. Doane prioritized engaging leadership, then driving a company-wide cultural shift toward modernization with support from AWS Professional Services.\nKCNSC used AWS Landing Zone Accelerator as the foundation for the unified cloud environment, then customized it to their needs. The project moved quickly: Doane presented the idea and successfully realized it in less than a year. The agency plans to use the enterprise blueprint to support other units in their cloud transformation journeys.\nNIF pursues AI to recover lost work hours Lawrence Livermore National Laboratory\u0026rsquo;s (LLNL) National Ignition Facility (NIF) owns some of the world\u0026rsquo;s highest-energy lasers, capable of initiating nuclear fusion reactions similar to those that power stars. (They even created a miniature star in their lab.) The facility uses 192 laser beams controlled by thousands of subsystems. Daily, scientists use NIF to conduct physics experiments critical to national security and global energy research.\nHowever, deteriorating infrastructure, loss of legacy system expertise as team members retired, and equipment failures were hindering research. Random system errors and lack of the integrated expertise needed to fix them effectively led to hundreds of lost test hours in 2024. LLNL needed an intelligent, scalable solution to troubleshoot equipment effectively and preserve fading legacy knowledge.\nShannon Ayers, Division Leader and Technical Manager at NIF, is part of the NIF Operations team, determined to solve these problems. They partnered with LLNL\u0026rsquo;s AI Initiative to use AWS Professional Services to integrate generative AI tools into NIF\u0026rsquo;s system search. The team used Amazon SageMaker to create a chat assistant using Retrieval Augmented Generation (RAG) technology to perform semantic search on NIF\u0026rsquo;s massive incident log database. The tool generates step-by-step decision trees, giving staff instant access to experiment records and equipment recovery spanning more than 20 years. Issues that previously delayed experiments for hours can now be resolved in minutes, significantly increasing efficiency.\n\u0026ldquo;AWS helped provide secure, scalable, FedRAMP-compliant infrastructure, and advanced AI tools like SageMaker and semantic search have accelerated our mission execution through improved efficiency, reduced technical debt by leveraging existing tools, and enhanced innovation by bringing together leading thinkers,\u0026rdquo; Ayers stated. \u0026ldquo;The collaboration with AWS has transformed our troubleshooting process and is key to continuous operational excellence at the National Ignition Facility,\u0026rdquo; she concluded.\nThis achievement will help NIF recover capacity to increase the number of experiments per year. In the future, the team plans to expand predictive maintenance capabilities and explore operational management tools to support the agency\u0026rsquo;s critical operations through 2040 and beyond.\nUSPTO sees immediate IT productivity increase using generative AI The United States Patent and Trademark Office (USPTO) grants patents and registers trademarks for creators. With increasing demand, the agency needed to modernize systems to provide more digital services without increasing resource usage. Like other agencies, USPTO\u0026rsquo;s legacy infrastructure had limited its ability to meet the needs of domestic creators.\nStephan Mitchev, former Chief Technology Officer of USPTO, had a vision to improve his division\u0026rsquo;s IT system efficiency. He used AWS to deploy chat assistants and code assistants into developer workflows, helping the team perform tasks like unit testing, documentation, and reverse engineering legacy code, resulting in an immediate 20% productivity increase. As of May 2025, Mitchev had brought 27 teams into the new workflow, all reporting positive results.\nMitchev\u0026rsquo;s advice to teams is to start small with AI tool deployments and measure results. He emphasized using AI to augment rather than replace current employees, and advised teams to measure and demonstrate return on investment (ROI) at each step to justify continued use.\nFederal agencies are using AWS to modernize systems, increase efficiency, and improve their ability to serve the public. Connect with your account team to learn how AWS can improve your organization\u0026rsquo;s operational efficiency.\nTAGS: Artificial Intelligence, AWS Public Sector, customer story, digital transformation, efficiencies, federal, government, migration, modernization\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Configure granular access permissions to Amazon Bedrock models using Amazon SageMaker Unified Studio By Emma Harrison on July 9, 2025 in Amazon SageMaker, Artificial Intelligence, Customer Solutions, Generative AI, Government, Migration, Public Sector Permalink Share\nFederal agencies are working to meet the demand for systems that can keep up with a tech-savvy public. Agency leaders are seizing the opportunity to restructure government by modernizing legacy systems and finding safer ways to serve citizens digitally. At the AWS Summit in Washington, DC 2025, federal leaders presented how they are using Amazon Web Services (AWS) to improve efficiency.\nAgencies are using cloud tools to increase productivity, minimize resource usage, and improve their ability to serve the public effectively. This article highlights pioneering leaders who are opening new paths in government cloud computing.\nFCC modernizes 94 percent of systems The Federal Communications Commission (FCC) was struggling with outdated, legacy infrastructure that was expensive and inefficient to maintain. Frequent hardware challenges, outdated application technology stacks, and increasing cybersecurity compliance challenges were putting pressure on the agency. High operating costs, limited scalability, and dependence on specialized skill sets to manage legacy systems were slowing progress. Don Tweedie, FCC Deputy Chief Information Officer for Technology Delivery, and his team recognized the need for a fundamental transformation.\nTweedie led this effort by building a strategic playbook to modernize the FCC\u0026rsquo;s network architecture. His vision focused on a cloud-first approach—prioritizing scalability, security, and flexibility. The agency began treating data as a strategic asset, implementing micro-segmentation, and leveraging native cloud features like auto-scaling and serverless computing. The result was a shift toward data-driven decision-making and a more flexible infrastructure.\nIn just 20 months, the FCC moved 94 percent of its data center operations to the cloud and decommissioned approximately 300 underutilized servers—streamlining the technology stack and reducing operational stagnation.\nTweedie emphasized the importance of starting with small, manageable pilot programs to build momentum and organizational buy-in. He recommended engaging security groups early, reinforcing zero-trust principles, upskilling current employees to address talent shortages, and establishing clear, actionable policies to guide the transformation.\nKCNSC bridges collaboration gap The Kansas City National Security Campus (KCNSC) operates on behalf of the National Nuclear Security Administration (NNSA) under the Department of Energy to protect the national nuclear program. With a distributed workforce of more than 7,000 people, the agency was increasingly struggling to build an effective collaborative environment in the unclassified space. The KCNSC team was grappling with inefficient data distribution, lengthy discussions about agency resource usage, and scaling challenges.\nTo address this urgent issue, Victor Doane, Enterprise Architect for Digital Transformation Program, pioneered the idea of a Joint Unclassified Cloud Environment (JUCE). He invited technical leaders and other groups to come together to identify organizational needs and discuss their different cloud transformation journeys to learn from each other\u0026rsquo;s experiences. Doane prioritized engaging leadership, then driving a company-wide cultural shift toward modernization with support from AWS Professional Services.\nKCNSC used AWS Landing Zone Accelerator as the foundation for the unified cloud environment, then customized it to their needs. The project moved quickly: Doane presented the idea and successfully realized it in less than a year. The agency plans to use the enterprise blueprint to support other units in their cloud transformation journeys.\nNIF pursues AI to recover lost work hours Lawrence Livermore National Laboratory\u0026rsquo;s (LLNL) National Ignition Facility (NIF) owns some of the world\u0026rsquo;s highest-energy lasers, capable of initiating nuclear fusion reactions similar to those that power stars. (They even created a miniature star in their lab.) The facility uses 192 laser beams controlled by thousands of subsystems. Daily, scientists use NIF to conduct physics experiments critical to national security and global energy research.\nHowever, deteriorating infrastructure, loss of legacy system expertise as team members retired, and equipment failures were hindering research. Random system errors and lack of the integrated expertise needed to fix them effectively led to hundreds of lost test hours in 2024. LLNL needed an intelligent, scalable solution to troubleshoot equipment effectively and preserve fading legacy knowledge.\nShannon Ayers, Division Leader and Technical Manager at NIF, is part of the NIF Operations team, determined to solve these problems. They partnered with LLNL\u0026rsquo;s AI Initiative to use AWS Professional Services to integrate generative AI tools into NIF\u0026rsquo;s system search. The team used Amazon SageMaker to create a chat assistant using Retrieval Augmented Generation (RAG) technology to perform semantic search on NIF\u0026rsquo;s massive incident log database. The tool generates step-by-step decision trees, giving staff instant access to experiment records and equipment recovery spanning more than 20 years. Issues that previously delayed experiments for hours can now be resolved in minutes, significantly increasing efficiency.\n\u0026ldquo;AWS helped provide secure, scalable, FedRAMP-compliant infrastructure, and advanced AI tools like SageMaker and semantic search have accelerated our mission execution through improved efficiency, reduced technical debt by leveraging existing tools, and enhanced innovation by bringing together leading thinkers,\u0026rdquo; Ayers stated. \u0026ldquo;The collaboration with AWS has transformed our troubleshooting process and is key to continuous operational excellence at the National Ignition Facility,\u0026rdquo; she concluded.\nThis achievement will help NIF recover capacity to increase the number of experiments per year. In the future, the team plans to expand predictive maintenance capabilities and explore operational management tools to support the agency\u0026rsquo;s critical operations through 2040 and beyond.\nUSPTO sees immediate IT productivity increase using generative AI The United States Patent and Trademark Office (USPTO) grants patents and registers trademarks for creators. With increasing demand, the agency needed to modernize systems to provide more digital services without increasing resource usage. Like other agencies, USPTO\u0026rsquo;s legacy infrastructure had limited its ability to meet the needs of domestic creators.\nStephan Mitchev, former Chief Technology Officer of USPTO, had a vision to improve his division\u0026rsquo;s IT system efficiency. He used AWS to deploy chat assistants and code assistants into developer workflows, helping the team perform tasks like unit testing, documentation, and reverse engineering legacy code, resulting in an immediate 20% productivity increase. As of May 2025, Mitchev had brought 27 teams into the new workflow, all reporting positive results.\nMitchev\u0026rsquo;s advice to teams is to start small with AI tool deployments and measure results. He emphasized using AI to augment rather than replace current employees, and advised teams to measure and demonstrate return on investment (ROI) at each step to justify continued use.\nFederal agencies are using AWS to modernize systems, increase efficiency, and improve their ability to serve the public. Connect with your account team to learn how AWS can improve your organization\u0026rsquo;s operational efficiency.\nTAGS: Artificial Intelligence, AWS Public Sector, customer story, digital transformation, efficiencies, federal, government, migration, modernization\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Summary Report: \u0026ldquo;GenAI-powered App-DB Modernization workshop\u0026rdquo; Event Objectives Share best practices in modern application design Introduce Domain-Driven Design (DDD) and event-driven architecture Provide guidance on selecting the right compute services Present AI tools to support the development lifecycle Speakers Jignesh Shah – Director, Open Source Databases Erica Liu – Sr. GTM Specialist, AppMod Fabrianne Effendi – Assc. Specialist SA, Serverless Amazon Web Services Key Highlights Identifying the drawbacks of legacy application architecture Long product release cycles → Lost revenue/missed opportunities Inefficient operations → Reduced productivity, higher costs Non-compliance with security regulations → Security breaches, loss of reputation Transitioning to modern application architecture – Microservices Migrating to a modular system — each function is an independent service communicating via events, built on three core pillars:\nQueue Management: Handle asynchronous tasks Caching Strategy: Optimize performance Message Handling: Flexible inter-service communication Domain-Driven Design (DDD) Four-step method: Identify domain events → arrange timeline → identify actors → define bounded contexts Bookstore case study: Demonstrates real-world DDD application Context mapping: 7 patterns for integrating bounded contexts Event-Driven Architecture 3 integration patterns: Publish/Subscribe, Point-to-point, Streaming Benefits: Loose coupling, scalability, resilience Sync vs async comparison: Understanding the trade-offs Compute Evolution Shared Responsibility Model: EC2 → ECS → Fargate → Lambda Serverless benefits: No server management, auto-scaling, pay-for-value Functions vs Containers: Criteria for appropriate choice Amazon Q Developer SDLC automation: From planning to maintenance Code transformation: Java upgrade, .NET modernization AWS Transform agents: VMware, Mainframe, .NET migration Key Takeaways Design Mindset Business-first approach: Always start from the business domain, not the technology Ubiquitous language: Importance of a shared vocabulary between business and tech teams Bounded contexts: Identifying and managing complexity in large systems Technical Architecture Event storming technique: Practical method for modeling business processes Use event-driven communication instead of synchronous calls Integration patterns: When to use sync, async, pub/sub, streaming Compute spectrum: Criteria for choosing between VM, containers, and serverless Modernization Strategy Phased approach: No rushing — follow a clear roadmap 7Rs framework: Multiple modernization paths depending on the application ROI measurement: Cost reduction + business agility Applying to Work Apply DDD to current projects: Event storming sessions with business teams Refactor microservices: Use bounded contexts to define service boundaries Implement event-driven patterns: Replace some sync calls with async messaging Adopt serverless: Pilot AWS Lambda for suitable use cases Try Amazon Q Developer: Integrate into the dev workflow to boost productivity Event Experience Attending the \u0026ldquo;GenAI-powered App-DB Modernization\u0026rdquo; workshop was extremely valuable, giving me a comprehensive view of modernizing applications and databases using advanced methods and tools. Key experiences included:\nLearning from highly skilled speakers Experts from AWS and major tech organizations shared best practices in modern application design. Through real-world case studies, I gained a deeper understanding of applying DDD and Event-Driven Architecture to large projects. Hands-on technical exposure Participating in event storming sessions helped me visualize how to model business processes into domain events. Learned how to split microservices and define bounded contexts to manage large-system complexity. Understood trade-offs between synchronous and asynchronous communication and integration patterns like pub/sub, point-to-point, streaming. Leveraging modern tools Explored Amazon Q Developer, an AI tool for SDLC support from planning to maintenance. Learned to automate code transformation and pilot serverless with AWS Lambda to improve productivity. Networking and discussions The workshop offered opportunities to exchange ideas with experts, peers, and business teams, enhancing the ubiquitous language between business and tech. Real-world examples reinforced the importance of the business-first approach rather than focusing solely on technology. Lessons learned Applying DDD and event-driven patterns reduces coupling while improving scalability and resilience. Modernization requires a phased approach with ROI measurement; rushing the process can be risky. AI tools like Amazon Q Developer can significantly boost productivity when integrated into the current workflow. Some event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about application design, system modernization, and cross-team collaboration.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report This report documents my internship experience at Amazon Web Services Vietnam Co., Ltd. as an FCJ Cloud Intern from August 12, 2025 to December 9, 2025. Throughout this period, I had the opportunity to work on real-world cloud computing projects, develop technical skills, and contribute to meaningful initiatives in the AWS ecosystem.\nThis comprehensive report covers my weekly work activities, project proposals, technical translations, event participation, hands-on workshops, self-assessment, and reflections on my professional growth during this valuable learning journey.\nStudent Information Full Name: Do Nguyen Duy Khang\nPhone Number: 0933681718\nEmail: khangdndse182375@fpt.edu.vn\nUniversity: FPT University\nMajor: Information Technology\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: August 12, 2025 - December 9, 2025\nReport Content This internship report is organized into the following sections, each providing detailed insights into different aspects of my learning and contribution:\nWorklog - Weekly documentation of tasks, achievements, and learning milestones throughout the internship period\nProposal - Project proposals and technical solutions developed during the internship\nTranslated Blogs - Technical blog translations contributing to knowledge sharing in the AWS community\nEvents Participated - AWS events, workshops, and community activities attended\nWorkshop - Hands-on technical workshops completed, including building serverless REST APIs with AWS Lambda and API Gateway\nSelf-evaluation - Comprehensive self-assessment of performance, strengths, areas for improvement, and overall reflection\nSharing and Feedback - Reflections, feedback, and insights gained from the internship experience\nThis report serves as a comprehensive record of my professional development and contributions during my internship at AWS Vietnam.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Workshop Overview What is Serverless? Serverless computing is a cloud computing model where the cloud provider manages the infrastructure, automatically provisioning, scaling, and managing servers. You only pay for the compute time you consume.\nArchitecture Overview In this workshop, we\u0026rsquo;ll build a serverless REST API with the following architecture:\nClient Request ↓ API Gateway (REST API) ↓ Lambda Function (Business Logic) ↓ DynamoDB (Data Storage) Components 1. Amazon API Gateway Creates RESTful APIs Handles HTTP requests and responses Manages authentication and authorization Provides throttling and monitoring 2. AWS Lambda Serverless compute service Executes code in response to events Automatically scales based on traffic Pay only for compute time used 3. Amazon DynamoDB NoSQL database service Fully managed and serverless Fast and scalable Automatic scaling What We\u0026rsquo;ll Build A Task Management API with the following endpoints:\nPOST /tasks - Create a new task GET /tasks - List all tasks GET /tasks/{id} - Get a specific task PUT /tasks/{id} - Update a task DELETE /tasks/{id} - Delete a task Benefits of Serverless Architecture No server management - AWS handles all infrastructure Automatic scaling - Handles traffic spikes automatically Cost-effective - Pay only for what you use High availability - Built-in redundancy and fault tolerance Fast development - Focus on code, not infrastructure "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Set Up account - Read and take note of internship unit rules and regulations 08/9/2025 09/9/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master knowledge of network security in AWS (Security Groups, NACL). Understand VPC connectivity and network connection methods. Learn about load balancing with AWS ELB and Load Balancer types. Tasks to be implemented this week: Day Tasks Start Date Completion Date Documentation Source 2 - Learn about Security Groups: + Concepts and stateful characteristics + How to create and configure rules + Apply to Elastic Network Interface 08/11/2025 08/11/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn about Network ACL (NACL): + Concepts and stateless characteristics + Differences between NACL and Security Groups + Rule evaluation mechanism from top to bottom 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Hands-on Practice: + Create Security Group for EC2 instance + Configure NACL for subnet + Compare security effectiveness between 2 methods - Learn about VPC Flow Logs 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about VPC connectivity: + VPC Peering and limitations + Transit Gateway + Site-to-Site VPN + AWS Direct Connect 08/14/2025 08/14/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Elastic Load Balancing: + ELB concepts and types + Application Load Balancer (ALB) + Network Load Balancer (NLB) + Sticky Session and Health Check - Hands-on Practice: + Create ALB to distribute traffic + Configure Target Groups 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Gained clear understanding of Security Groups and NACL:\nSecurity Groups are stateful firewalls that only allow \u0026ldquo;allow\u0026rdquo; rules NACL is a stateless firewall applied to subnets Security Groups affect individual instances, NACL affects multiple servers in a subnet Learned how to use VPC Flow Logs to monitor IP traffic in VPC without capturing packet contents.\nUnderstood VPC connection methods:\nVPC Peering to connect 2 VPCs, does not support transitive routing Transit Gateway to connect multiple VPCs and on-premises networks Site-to-Site VPN with Virtual Private Gateway and Customer Gateway AWS Direct Connect with low latency (20-30ms) Mastered knowledge of Elastic Load Balancing:\n4 types: ALB, NLB, Classic LB, Gateway LB ALB operates at Layer 7, supports HTTP/HTTPS and path-based routing NLB operates at Layer 4, supports TCP/TLS and static IP Features: Health Check, Sticky Session, Access Logs Successfully completed hands-on practice:\nCreated Security Groups for EC2 with appropriate rules Configured NACL for subnet with logical rule ordering Set up Application Load Balancer with Target Groups Verified Load Balancer operation and traffic distribution Gained ability to compare and select appropriate security solutions and network connectivity for specific use cases.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Get to know First Cloud Journey members Understand fundamental AWS services and master the use of AWS Management Console \u0026amp; AWS CLI Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Meet and greet FCJ members - Read and take notes on internship rules and regulations 08/11/2025 08/11/2025 3 - Overview of AWS and its service categories (Compute, Storage, Networking, Database, etc.) 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn AWS Management Console \u0026amp; AWS CLI - Hands-on: account creation, install \u0026amp; configure AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deep dive into Amazon EC2 (Instance types, AMI, EBS, Key Pairs, Instance Store, User Data, Metadata, Pricing models, Auto Scaling, etc.) - Related services: Lightsail, EFS, FSx, AWS MGN 08/14/2025 08/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - Hands-on practice: ∘ Launch EC2 instances ∘ Connect via SSH/RDP ∘ Attach EBS volume ∘ Create snapshots \u0026amp; custom AMIs 08/15/2025 08/16/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Successfully completed 100% of the planned tasks with the following key takeaways:\nDeveloped comprehensive understanding of Amazon EC2 as scalable virtual servers capable of fully replacing physical servers for nearly any workload (web hosting, applications, databases, authentication, etc.).\nAcquired proficiency in hardware configuration through Instance Types (CPU, memory, network, storage) and related concepts: Hypervisor (Nitro/KVM/HVM/PV), Placement Groups, and Availability Zones.\nGained clear comprehension of AMI (Amazon Machine Image): includes root volume, launch permissions + block device mapping; can utilize AWS-provided, Marketplace, or custom AMIs; backup through AMI creation or EBS snapshots.\nAchieved expertise with Key Pairs: SSH authentication for Linux and encrypted Administrator password decryption for Windows instances.\nAttained thorough knowledge of Amazon EBS (Elastic Block Store):\nPersistent block storage independent of EC2 lifecycle, connected via private EBS network within the same AZ\nSSD \u0026amp; HDD families, 99.99% availability through replication across multiple nodes\nMulti-Attach capability on Nitro-based instances\nBackup via incremental snapshots stored in S3\nUnderstood Instance Store: high-performance ephemeral NVMe storage physically attached to the host; data lost on Stop (but preserved on Reboot); ideal for cache, buffer, swap, temporary data.\nLearned User Data scripts (Bash/PowerShell) and Instance Metadata for automation and self-configuration.\nExplored EC2 pricing models and Auto Scaling:\nOn-Demand, Reserved Instances, Savings Plans, Spot Instances\nAuto Scaling Groups support multiple pricing models, multi-AZ operation, and automatic registration with Elastic Load Balancers\nStudied complementary services:\nAmazon Lightsail: low-cost, simplified VPS with bundled data transfer, ideal for dev/test and light workloads\nEFS (Elastic File System): fully managed NFS for Linux, pay-for-use storage, supports on-premises access via DX/VPN\nFSx: managed Windows File Server (SMB) and FSx for Lustre, with built-in deduplication (30–50% savings)\nAWS Application Migration Service (MGN): continuous replication for lift-and-shift migration and DR from on-premises/physical/virtual servers to AWS\nHands-on success: launched EC2 instances, connected via SSH/RDP, attached EBS volumes, created snapshots and custom AMIs.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master Amazon Simple Storage Service (S3) and its ecosystem Understand object storage concepts, storage classes, lifecycle management, security, and performance optimization Explore the AWS Snow Family and hybrid storage solutions Learn disaster recovery concepts (RTO/RPO) and backup strategies Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1–2 Deep dive into Amazon S3: architecture, durability, availability, storage classes, lifecycle policies, versioning, static website hosting, CORS, access control (ACL, Bucket Policy, IAM), Access Points, endpoints 08/18–08/19 08/19/2025 https://cloudjourney.awsstudygroup.com/ 3 S3 performance optimization, multipart upload, prefix design, event notifications, replication (CRR/SRR) 08/20/2025 08/20/2025 4 Amazon S3 Glacier (Instant Retrieval, Flexible Retrieval, Deep Archive), retrieval options (Expedited, Standard, Bulk) 08/21/2025 08/21/2025 5 AWS Snow Family (Snowball, Snowball Edge, Snowmobile) and AWS Storage Gateway (File, Volume, Tape) 08/22/2025 08/22/2025 6 Disaster recovery concepts (RTO, RPO), 4 DR strategies on AWS, AWS Backup service 08/23/2025 08/23/2025 Week 4 Achievements: Successfully completed all Week 4 objectives with the following key learnings:\nAttained proficiency in Amazon S3 fundamentals:\nObject storage (not block/file), write-once-read-many (WORM) model, unlimited total capacity, max 5 TB per object\nDesigned for 99.999999999% (11×9s) durability and 99.99% availability\nData automatically replicated across minimum 3 Availability Zones in a region\nSupports multipart upload, event notifications, static website hosting, and CORS configuration\nUnderstood S3 Storage Classes and cost optimization:\nS3 Standard, Standard-IA, One Zone-IA, Intelligent-Tiering, Glacier Instant Retrieval, Glacier Flexible Retrieval, Glacier Deep Archive\nLifecycle policies to automatically transition or expire objects\nSecurity \u0026amp; access control:\nS3 Bucket Policies, IAM Policies, S3 Access Control Lists (legacy), S3 Access Points\nBlock Public Access, VPC Endpoints for private connectivity\nAdvanced features:\nVersioning (protects against accidental deletion/overwrites and ransomware)\nCross-Region Replication (CRR) \u0026amp; Same-Region Replication (SRR)\nS3 performance optimization using random prefixes to distribute objects across partitions\nDeveloped comprehensive understanding of Amazon S3 Glacier:\nLow-cost archival storage with three retrieval options:\nExpedited (1–5 min)\nStandard (3–5 hours)\nBulk (5–12 hours)\nLearned AWS Snow Family for large-scale data migration:\nSnowball (80 TB), Snowball Edge (100 TB + compute), Snowmobile (up to 100 PB per truck) Mastered AWS Storage Gateway – hybrid storage:\nFile Gateway → S3 (NFS/SMB)\nVolume Gateway → S3 (iSCSI, cached or stored mode, snapshot to EBS)\nTape Gateway → Virtual Tape Library (S3/Glacier)\nDisaster Recovery concepts:\nRTO (Recovery Time Objective) – maximum acceptable downtime\nRPO (Recovery Point Objective) – maximum acceptable data loss\nFour AWS DR strategies: Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-Site Active/Active\nAWS Backup – centralized backup management across EBS, EC2, RDS, DynamoDB, EFS, Storage Gateway\nHands-on practice completed:\nCreated S3 buckets with versioning, lifecycle policies, static website hosting\nConfigured bucket policies, CORS, and VPC endpoints\nTested multipart upload and event triggers\nExplored AWS Backup console and created backup plans\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Master the AWS Shared Responsibility Model Deep understanding of identity and access management on AWS Gain proficiency in IAM, Organizations, Identity Center, Cognito, KMS, and Security Hub Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1–2 Shared Responsibility Model + IAM fundamentals (Root, Users, Groups, Policies, Roles) 25–26/08/2025 26/08/2025 https://cloudjourney.awsstudygroup.com/ 3 IAM Policies (Identity-based vs Resource-based), Policy Evaluation Logic, Explicit Deny 27/08/2025 27/08/2025 4 IAM Roles, Trust Policy, STS, AssumeRole, Cross-account access, Service roles 28/08/2025 28/08/2025 5 AWS Organizations, OU, SCP, Consolidated Billing + AWS Identity Center 29/08/2025 29/08/2025 6 Amazon Cognito, AWS KMS, AWS Security Hub 30/08/2025 30/08/2025 Week 5 Achievements: Successfully completed all Week 5 objectives with strong mastery of:\nAWS Shared Responsibility Model: AWS secures the cloud, customers secure in the cloud. Responsibility varies by service type (Infrastructure → Container → Abstracted services).\nRoot Account Protection: MFA enabled, never used for daily tasks, credentials safely stored, IAM Administrator users created instead.\nAWS IAM Mastery:\nPrincipals: Root, IAM Users, IAM Roles, Federated Users, AWS Services\nPolicy types and evaluation: Explicit Deny wins, then Allow, then default Deny\nIAM Roles + Trust Policy + STS for temporary, least-privilege, cross-account access\nCross-account delegation and service-linked roles\nAWS Organizations:\nCentralized multi-account management with OUs\nService Control Policies (SCP) to set permission guardrails\nConsolidated Billing\nAWS Identity Center (formerly AWS SSO):\nIdentity source integration (built-in, AWS Managed Microsoft AD, AD Connector, external IdP)\nPermission Sets → auto-provisioned IAM Roles in member accounts\nAmazon Cognito:\nUser Pools for user sign-up/sign-in\nIdentity Pools for temporary AWS credential issuance\nAWS KMS:\nCustomer Managed Keys (CMK), FIPS 140-2 compliant\nData Key pattern for large-scale encryption\nAWS Security Hub:\nContinuous security checking against AWS Foundational Best Practices, CIS, PCI DSS, etc.\nSecurity score and prioritized findings\nHands-on completed:\nBuilt full AWS Organizations structure with SCPs\nConfigured AWS Identity Center with Permission Sets\nImplemented Cognito authentication flow\nEncrypted resources with KMS\nActivated and analyzed Security Hub findings\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master core and advanced database concepts Clearly distinguish OLTP vs OLAP, RDBMS vs NoSQL Gain full proficiency in AWS database services: RDS, Aurora, Redshift, ElastiCache Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1–2 Core DB concepts: PK, FK, Index, Partition, Query Plan, Buffer, Log, Session 01–02/09/2025 02/09/2025 https://cloudjourney.awsstudygroup.com/ 3 RDBMS vs NoSQL • OLTP vs OLAP 03/09/2025 03/09/2025 4 Amazon RDS \u0026amp; Amazon Aurora (MySQL/PostgreSQL-compatible) 04/09/2025 04/09/2025 5 Amazon Redshift – Managed Data Warehouse \u0026amp; OLAP 05/09/2025 05/09/2025 6 Amazon ElastiCache (Redis \u0026amp; Memcached) 06/09/2025 06/09/2025 Week 6 Achievements: Successfully completed all Week 6 objectives with strong mastery of:\nCore database concepts: Primary Key, Foreign Key, Index, Partitioning, Execution Plan, Buffer Pool, Transaction Log, Session\nClear differentiation between:\nRDBMS (relational, SQL, ACID) vs NoSQL (flexible schema, eventual consistency)\nOLTP (fast transactions, row-based) vs OLAP (complex analytics, columnar, historical data)\nAmazon RDS:\nFully managed relational databases (MySQL, PostgreSQL, MariaDB, Oracle, SQL Server, Aurora)\nAutomated backups, Read Replicas, Multi-AZ failover, storage autoscaling, encryption at rest \u0026amp; in transit\nAmazon Aurora:\nCloud-native relational database with MySQL \u0026amp; PostgreSQL compatibility\nRevolutionary storage layer for high concurrent read/write performance\nUnique features: Backtrack, Aurora Clones, Global Database, Multi-Master\nAmazon Redshift:\nFully managed petabyte-scale data warehouse optimized for OLAP\nMPP architecture + columnar storage\nLeader + Compute nodes, Redshift Spectrum, concurrency scaling\nAmazon ElastiCache:\nManaged Redis \u0026amp; Memcached\nAutomatic failure detection and replacement\nUsed as caching layer in front of databases to offload read-heavy OLTP workloads\nRedis preferred for new applications\nHands-on completed:\nDeployed RDS Multi-AZ + Read Replica\nCreated Aurora cluster with Global Database\nBuilt Redshift cluster and ran analytical queries\nDeployed ElastiCache Redis and integrated with sample app\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Initiate Human Resource Management System project with Face Recognition Attendance Conduct requirements analysis and project planning Research and select appropriate technology stack Set up development environment and project structure Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Project requirements analysis - Define core features and functionalities - Research technology stack options 08/18/2025 08/18/2025 2 - Finalize technology stack: React 19, Vite, TailwindCSS, Flask - Design system architecture (Frontend/Backend separation) - Create project structure 08/19/2025 08/19/2025 3 - Set up development environment - Initialize React project with Vite - Configure TailwindCSS and basic dependencies 08/20/2025 08/20/2025 4 - Set up Python Flask backend structure - Install required Python packages (face_recognition, opencv, flask) - Configure project directories 08/21/2025 08/21/2025 5 - Create database schema design - Design API endpoints structure - Set up version control (Git) 08/22/2025 08/22/2025 6 - Document project setup and architecture - Create initial README with project overview - Plan development timeline for remaining weeks 08/23/2025 08/23/2025 Week 7 Achievements: Successfully initiated the HR Management System project with comprehensive planning:\nCompleted thorough requirements analysis covering:\nEmployee Management system requirements Face Recognition Attendance system specifications Payroll Management features User Management and role-based access control Reporting and Analytics needs Selected and finalized technology stack:\nFrontend: React 19 with Vite for modern development experience Styling: TailwindCSS for utility-first CSS approach Routing: React Router for client-side navigation Icons: Lucide React for consistent iconography Animations: Framer Motion for smooth UI transitions Backend: Python Flask for RESTful API Face Recognition: face_recognition library with OpenCV Established project architecture:\nSeparated frontend and backend into independent modules Designed scalable folder structure for components, pages, services Planned API communication patterns between frontend and backend Set up complete development environment:\nInitialized React project with Vite configuration Configured TailwindCSS with custom theme settings Installed and configured all necessary dependencies Set up Python virtual environment for backend Created backend foundation:\nStructured Flask application with modular design Installed face recognition libraries and dependencies Configured project directories (datasets, trainer, attendance, logs) Set up Docker configuration for containerization Designed database schema:\nEmployee information tables Attendance records structure User authentication and roles Payroll and task management schemas Established development workflow:\nInitialized Git repository with proper .gitignore Created comprehensive README documentation Planned 6-week development timeline Set up project management and task tracking Prepared for agile development approach with clear milestones and deliverables for subsequent weeks.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Build frontend foundation with React 19 and Vite Implement routing structure and layout components Create reusable UI components with TailwindCSS Establish API service layer for backend communication Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Set up React Router for navigation - Create main layout components (Header, Sidebar, Footer) - Implement responsive design structure 08/25/2025 08/25/2025 2 - Build reusable UI components (Button, Input, Card, Modal) - Implement dark/light theme toggle - Create loading and error state components 08/26/2025 08/26/2025 3 - Set up API service layer with axios/fetch - Create API endpoints configuration - Implement error handling and response interceptors 08/27/2025 08/27/2025 4 - Build Dashboard page with overview widgets - Create navigation menu structure - Implement page routing and protected routes 08/28/2025 08/28/2025 5 - Add Framer Motion animations to components - Integrate Lucide React icons throughout UI - Polish UI/UX with smooth transitions 08/29/2025 08/29/2025 6 - Test responsive design on multiple devices - Optimize component performance - Document component usage and props 08/30/2025 08/30/2025 Week 8 Achievements: Successfully established frontend foundation with modern React architecture:\nImplemented comprehensive routing system:\nConfigured React Router with nested routes Created protected route components for authentication Set up navigation guards and route transitions Implemented dynamic route parameters Built complete layout structure:\nResponsive Header component with user menu Collapsible Sidebar navigation with active state indicators Footer component with system information Mobile-responsive design with hamburger menu Developed reusable UI component library:\nButton component with multiple variants (primary, secondary, danger) Input components with validation states Card components for content display Modal/Dialog components for overlays Loading spinners and skeleton screens Error boundary and error display components Established API communication layer:\nCreated centralized API service with axios Implemented request/response interceptors Set up environment variable configuration Built error handling and retry mechanisms Created API endpoint constants and types Designed and implemented Dashboard:\nOverview widgets showing key metrics Quick access cards for main features Recent activity feed Statistics visualization placeholders Integrated modern UI enhancements:\nFramer Motion animations for smooth page transitions Lucide React icons throughout the application Dark/Light theme toggle functionality Responsive breakpoints for mobile, tablet, desktop Optimized application performance:\nCode splitting with React.lazy() Component memoization where appropriate Optimized bundle size with Vite Implemented loading states for better UX Created comprehensive documentation:\nComponent usage guidelines API integration patterns Styling conventions with TailwindCSS Development workflow documentation "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Implement Employee Management module with CRUD operations Develop User Management system with role-based access control Create authentication and authorization mechanisms Build employee profile and information management features Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Design Employee data model and API endpoints - Create Employee List page with table view - Implement search and filter functionality 09/01/2025 09/01/2025 2 - Build Add/Edit Employee form with validation - Implement employee profile view - Create employee detail page 09/02/2025 09/02/2025 3 - Develop User Management module - Implement role-based access control (Admin/User) - Create user registration and authentication 09/03/2025 09/03/2025 4 - Build login/logout functionality - Implement session management - Create protected routes based on user roles 09/04/2025 09/04/2025 5 - Integrate frontend with backend API - Test CRUD operations for employees - Validate user authentication flow 09/05/2025 09/05/2025 6 - Add pagination and sorting to employee list - Implement bulk operations (delete, export) - Polish UI/UX for employee management 09/06/2025 09/06/2025 Week 9 Achievements: Successfully implemented core HR management modules:\nDeveloped comprehensive Employee Management system:\nEmployee List page with advanced table features (sorting, filtering, pagination) Search functionality across multiple employee fields Add Employee form with comprehensive validation Edit Employee functionality with pre-populated data Employee detail view with complete information display Delete employee with confirmation modal Bulk operations for multiple employee management Built robust User Management module:\nUser registration with role assignment User list with role-based filtering Edit user roles and permissions User profile management Account activation/deactivation Implemented authentication and authorization:\nLogin page with credential validation JWT token-based authentication Session management with secure storage Automatic token refresh mechanism Logout functionality with session cleanup Created role-based access control (RBAC):\nAdmin role with full system access User role with limited permissions Protected routes based on user roles Permission-based UI component rendering Access control middleware Integrated frontend with backend:\nRESTful API integration for employee operations Error handling and user feedback Loading states for async operations Optimistic UI updates Form validation on both client and server side Enhanced user experience:\nResponsive employee management interface Real-time form validation Success/error notifications Confirmation dialogs for destructive actions Smooth transitions and animations Implemented data management features:\nEmployee data export functionality Import employee data from CSV Employee profile image upload Data pagination for large datasets Advanced filtering options "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Connect and get acquainted with members of First Cloud Journey and understand basic AWS services\nWeek 2: Master knowledge of network security in AWS (Security Groups, NACL) and load balancing with AWS ELB\nWeek 3: Get to know First Cloud Journey members and understand fundamental AWS services\nWeek 4: Master Amazon Simple Storage Service (S3) and its ecosystem\nWeek 5: Master the AWS Shared Responsibility Model and identity and access management on AWS\nWeek 6: Master core and advanced database concepts and gain proficiency in AWS database services\nWeek 7: Initiate Human Resource Management System project with Face Recognition Attendance\nWeek 8: Build frontend foundation with React 19 and Vite, implement routing structure and layout components\nWeek 9: Implement Employee Management module with CRUD operations and User Management system\nWeek 10: Implement Face Recognition system backend with Python Flask and face detection algorithms\nWeek 11: Integrate Attendance Tracking with face recognition system and develop Payroll Management module\nWeek 12: Develop Reports \u0026amp; Analytics module with data visualization and implement Document Management system\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS GenAI Builders Club\u0026rdquo; Event Objectives The event aimed to explore AI-driven development lifecycle (AI DLC) and introduce modern AI-powered development tools, focusing on how AI is transforming software development practices and how developers can effectively integrate AI into their workflow.\nSpeakers Toàn Huỳnh - Senior Specialist SA (Solutions Architect)\nTopic: AI-driven Development Lifecycle My Nguyễn - Senior Prototyping Architect\nTopic: Kiro - The AI IDE for Prototype to Production Key Highlights AI-Driven Development Lifecycle (by Toàn Huỳnh) AI Evolution in Software Development:\n2023: AI helping developers write code faster\n2024: AI generating larger pieces of code and answering faster\n2025: AI completing development tasks end-to-end with human in the loop\nKey Principles:\nAI DLC is not just a tool - it\u0026rsquo;s a methodology\nAI should be used for pair-programming, not as a standalone solution\nDevelopers must remain the owner and validate all AI-generated code\nQuality control is the developer\u0026rsquo;s responsibility\nChallenges with AI at Scale:\nQuality control issues when generating large amounts of code\nLoss of control and understanding of what AI is doing\nRisk of having to restart projects if quality is not maintained\nToken overuse in large projects\nAI DLC Workflow:\nAI should not make decisions independently - developers make all decisions\nUse AI to analyze problems, review requirements, and create plans\nStore results in Markdown files for continuity\nBreak down large projects into smaller units\nRegular review and validation are essential\nUse AI for standardized tasks, handle thinking tasks manually\nBest Practices:\nStart with simple requirements and break them into units\nUse AI to group important units for end users\nImplement each unit as a small project\nMaintain a shared track for backend and frontend collaboration\nFor large projects, consider using Amazon Q instead of simple AI tools\nKiro - The AI IDE (by My Nguyễn) Overview:\nKiro is an AI-powered IDE designed for prototype to production development\nKey differences between Kiro and VSCode were discussed\nFocus on streamlining the development process from ideation to deployment\nKey Workflow Features:\nRole Separation: Business → Architecture → Implementation progression\nAI-Enhanced: Each role has specialized AI context and capabilities\nIterative: Feedback loops within each stage\nTemplate-Driven: Standardized outputs using AIDLC templates\nAgent Hooks:\nDelegate tasks to AI agents that trigger on events such as \u0026lsquo;file save\u0026rsquo;\nAgents autonomously execute in the background based on your pre-defined prompts\nAgent hooks help you scale your work by generating documentation, unit tests, or optimizing code performance\nMaximize Semantics per Token:\nAI doesn\u0026rsquo;t \u0026rsquo;think in code\u0026rsquo; – it reasons in meaning\nDon\u0026rsquo;t dump 10K lines of source; Instead feed semantic-rich intermediate models\nTrim the artefacts (ex. user stories, component models) to be crisply semantic; no extras!\nRemember the Tokens/Semantics Ratio\nKey Takeaways AI as a Partner, Not a Replacement:\nView AI as a collaborative partner in problem-solving\nMaintain human oversight and decision-making authority\nQuality assurance remains a human responsibility\nStructured AI Workflow:\nDefine clear roles and responsibilities in prompts\nUse Markdown files to track progress and maintain context\nBreak down complex projects into manageable units\nImplement regular review cycles\nQuality Control:\nNever fully delegate tasks to AI without validation\nReview AI-generated code and plans regularly\nBuild documentation (Markdown files) to avoid going off track\nUse AI for standardized tasks, not critical thinking\nProject Management:\nStart with simple requirements and scope down\nUse AI to group important units for end users\nTreat each unit as a separate small project\nMaintain shared communication channels for team collaboration\nTool Selection:\nFor large projects, consider enterprise solutions like Amazon Q\nSimple AI tools are suitable for smaller, well-defined tasks\nChoose tools based on project complexity and requirements\nApplying to Work Implement AI DLC Methodology:\nAdopt a structured approach to AI-assisted development\nCreate Markdown documentation for project planning and tracking\nEstablish clear workflows for AI collaboration\nQuality Assurance:\nAlways review and validate AI-generated code\nImplement regular checkpoints in the development process\nMaintain ownership of all deliverables\nProject Breakdown:\nBreak large projects into smaller, manageable units\nUse AI to help identify and group important components\nTrack progress using checkboxes in planning documents\nTeam Collaboration:\nEstablish shared tracks for backend and frontend teams\nUse AI to facilitate communication and planning\nMaintain clear documentation for team alignment\nTool Evaluation:\nEvaluate AI tools based on project needs\nConsider enterprise solutions for large-scale projects\nUse appropriate tools for different development phases\nEvent Experience The event provided valuable insights into the practical application of AI in software development. The speakers emphasized the importance of maintaining human control and oversight while leveraging AI capabilities. The discussion on AI DLC methodology was particularly enlightening, showing how to structure AI collaboration effectively.\nThe presentation on Kiro offered a glimpse into next-generation development tools that integrate AI directly into the IDE, potentially streamlining the development process from prototype to production.\nThe key lesson learned is that AI should enhance developer capabilities rather than replace critical thinking and decision-making processes. Successful AI integration requires careful planning, regular review, and maintaining clear ownership of the development process.\nSome event photos Overall, the event not only provided technical knowledge but also helped me reshape my thinking about AI-driven development lifecycle and how to effectively integrate AI tools into the development workflow while maintaining quality and control.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Expand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequisites and Setup","tags":[],"description":"","content":"Prerequisites Required AWS Services Access To complete this workshop, you need access to the following AWS services:\nAWS Lambda Amazon API Gateway Amazon DynamoDB AWS IAM Amazon CloudWatch Logs IAM Permissions Your AWS account needs the following IAM permissions:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;dynamodb:*\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Software Requirements AWS Account with appropriate permissions AWS Console Access or AWS CLI configured Basic knowledge of Python 3.x Text editor or IDE (VS Code, PyCharm, etc.) AWS Region We\u0026rsquo;ll use us-east-1 (N. Virginia) for this workshop. You can choose any region, but make sure all resources are created in the same region.\nSetup Steps Step 1: Verify AWS Access Log in to the AWS Management Console Navigate to the AWS Lambda service Ensure you can see the Lambda dashboard Step 2: Choose Your Region Select your preferred AWS region from the top-right corner Remember this region - all resources will be created here Step 3: Prepare Your Environment Have your text editor ready for writing Lambda function code Keep the AWS Console open in your browser Optionally, have AWS CLI configured if you prefer command-line tools Estimated Costs This workshop uses AWS Free Tier eligible services:\nLambda: 1M free requests per month API Gateway: 1M API calls per month for REST APIs DynamoDB: 25GB storage, 25 read/write capacity units Note: If you exceed free tier limits, you may incur minimal charges. Always clean up resources after completing the workshop.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Enterprise HR Management System Comprehensive HR Management Solution for Modern Enterprises 1. Executive Summary Enterprise HR Management System is an integrated HR management solution designed for mid-sized enterprises in Vietnam, supporting 100-500 employees. The system automates the entire HR workflow from profile management, attendance tracking, payroll calculation to performance evaluation. This is an in-house project developed by the team, focusing on MVP with optimized costs under $100/month in the initial phase (100 employees), utilizing AWS serverless architecture with Lambda, API Gateway, DynamoDB to ensure high performance and low costs.\n2. Problem Statement Current Issues Vietnamese enterprises use Excel or legacy HR software, causing time waste and errors. Manual processes (attendance, payroll) are not integrated. No automated approval workflows. Difficult to manage detailed permissions. Weak reporting, not real-time. High costs for SAP, Workday solutions. Proposed Solution The system uses AWS Serverless Architecture to optimize costs:\nCompute: AWS Lambda (pay-per-use, no idle costs). API: API Gateway REST API. Database: DynamoDB (on-demand billing). Cache: ElastiCache Redis (cache.t3.micro) - optional for phase 2. Authentication: AWS Cognito (free tier \u0026lt;50K MAU). Storage: S3 for documents, CloudFront CDN. CI/CD: GitHub Actions for automated deployment. Monitoring: CloudWatch (free tier). Security: Route 53, WAF (cost-optimized rules), IAM Roles. Key Features Single Sign-On (Google, Microsoft 365). Detailed RBAC (Admin, Manager, Employee, Payroll Officer). Check-in/out with GPS validation. Automated payroll calculation with flexible formulas. Approval workflows (leave, salary adjustment). Mobile app (React Native) for attendance. Real-time reporting dashboard. Comprehensive audit logs. Benefits Save 70% of manual HR processing time. Reduce 90% of data entry errors. Cost only $45-70/month for 100 employees (90% cheaper than SAP/Workday). In-house development - no outsourcing costs. 3. Solution Architecture Here is the cloud architecture diagram of the system:\nAWS Services Used AWS Service Primary Function AWS Lambda Backend API logic (Node.js 20.x) API Gateway REST API endpoints, request validation Amazon DynamoDB NoSQL database (on-demand billing) AWS Cognito Authentication, SSO (Google/Microsoft), JWT tokens Amazon S3 Document storage (CV, contracts, payslips) CloudFront CDN for static assets and S3 Route 53 DNS management AWS WAF (optional Phase 2) API protection CloudWatch Logs, monitoring (free tier) Secrets Manager API keys, credentials Component Design Authentication Layer Cognito User Pools with JWT (RS256). Lambda authorizer for API Gateway. Optional MFA (SMS/TOTP) - Phase 2. API Layer AWS Lambda functions (Node.js) deployed via GitHub Actions. API Gateway REST API with resource-based routing. Rate limiting (10 requests/second). CORS configured for web/mobile. Business Logic (Lambda Functions) Employee management (CRUD, contracts, skills). Attendance tracking (check-in/out, GPS validation). Leave management (requests, approvals, balance). Payroll engine (salary calculation, tax, insurance). Performance reviews (KPI tracking). Email notifications (SES free tier). Data Layer - DynamoDB Tables Users - GSI on email Employees - GSI on department_id Departments AttendanceLogs - GSI on employee_id + date LeaveRequests - GSI on employee_id + status PayrollRecords - GSI on employee_id + month Approvals - GSI on approver_id + status Storage Layer S3 Standard for new documents (\u0026lt;30 days). S3 Lifecycle → Glacier Deep Archive (\u0026gt;90 days). Presigned URLs for secure upload/download. CloudFront distribution for static web hosting. Frontend Next.js 14 (React 18) + TypeScript - Static export. Material-UI components. Hosted on CloudFront + S3 (no server cost). Mobile app: React Native (Expo) with AsyncStorage. CI/CD Pipeline GitHub Actions workflow: Build Lambda functions → ZIP packages Deploy to Lambda via AWS CLI Update API Gateway configurations Deploy frontend to S3 Automated Jest unit tests. 4. Technical Implementation Phase 1: MVP Core (Month 1-2) Month 1: AWS setup (Cognito, DynamoDB tables, S3, Lambda). Authentication + Login UI. Employee CRUD APIs + admin dashboard. Month 2: Attendance check-in/out APIs with GPS. Mobile app MVP (React Native). Leave request workflow. Basic reporting dashboard. Phase 2: Payroll \u0026amp; Automation (Month 3-4) Month 3: Payroll calculation engine (Lambda). Payslip generation (PDF via Lambda layer). Approval workflows. Month 4: Email notifications (SES). Audit logging to DynamoDB. Export reports (CSV). Performance optimization. Phase 3: Advanced Features (Month 5-6) Performance review module. Training tracking. Advanced analytics dashboard. Security hardening. Load testing \u0026amp; optimization. User training \u0026amp; documentation. Tech Stack Component Technology/Service Backend Node.js 20.x, AWS Lambda, AWS SDK v3 Database DynamoDB (single-table design pattern) Frontend Next.js 14, React 18, TypeScript, Material-UI v5 Mobile React Native (Expo), AsyncStorage Infrastructure as Code AWS SAM / Serverless Framework CI/CD GitHub Actions 5. Roadmap \u0026amp; Milestones Month Phase Key Deliverables 1-2 MVP Core Auth, Employee management, Attendance mobile app 3-4 Payroll \u0026amp; Automation Payroll engine, approval workflows, notifications 5-6 Advanced \u0026amp; Launch Analytics, performance reviews, UAT, go-live 6. Budget Estimation Monthly AWS Costs (Phase 1: 100 employees, ~5,000 API calls/day) Serverless Architecture - Cost Optimized Service Configuration Cost/Month AWS Lambda 150K invocations, 512MB, 500ms avg $0 ↳ Free tier: 1M requests + 400K GB-seconds/month (Within free tier) API Gateway 150K REST API requests/month $0.15 ↳ $3.50 per million after first 1M (free tier year 1) DynamoDB On-demand, 5GB storage, 1M reads, 500K writes $3.50 ↳ Storage: $1.25/GB ($6.25) + Reads: $0.25/M + Writes: $1.25/M S3 Storage 20GB documents (100 users) $0.46 S3 Requests 20K PUT, 100K GET/month $0.14 S3 Glacier (archive) 10GB old documents $0.10 CloudFront 10GB transfer, 200K requests $1.00 Route 53 1 hosted zone + 1M queries $0.90 CloudWatch Logs 2GB logs/month $0 ↳ (First 5GB free) (Within free tier) Secrets Manager 2 secrets $0.80 SES (email) 500 emails/month $0.05 Cognito \u0026lt;50K MAU $0 ↳ (Free tier) (Within free tier) Data Transfer OUT 5GB to internet $0.45 Contingency (10%) Buffer $0.75 TOTAL AWS/MONTH (100 users) ~$8.30 Costs When Scaling to 200 Users (Phase 2) Service Changes Cost/Month Lambda 300K invocations (still in free tier) $0 API Gateway 300K requests $0.30 DynamoDB 10GB, 2M reads, 1M writes $9.50 S3 + CloudFront 40GB storage, 20GB transfer $2.50 Route 53, Secrets, SES, Transfer (similar) $2.20 ElastiCache Redis cache.t3.micro (optional) $12.50 AWS WAF Basic protection (optional) $7.00 Contingency $3.40 TOTAL (200 users, with cache + WAF) ~$37.40 TOTAL (200 users, without cache/WAF) ~$17.90 Costs When Scaling to 500 Users (Phase 3) | Lambda + API Gateway | 750K invocations | $3.50 | | DynamoDB | 25GB, 5M reads, 2.5M writes | $32.50 | | S3 + CloudFront + Transfer | 100GB storage, 50GB CDN | $7.50 | | ElastiCache Redis | cache.t3.small | $25.00 | | AWS WAF | 2 rules | $8.00 | | Route 53, Secrets, SES, misc | | $3.00 | | Contingency | | $8.00 | | | | | | TOTAL (500 users) | | ~$87.50 |\nHosting Cost Summary by Phase Phase Users Cost/Month Cost/Year Phase 1 MVP 100 $8-12 ~$100-150 Phase 2 Growth 200 $18-38 ~$220-450 Phase 3 Scale 500 $88-95 ~$1,050 Development Costs (In-house team - NO outsourcing cost) Assumption: In-house team already has fixed salaries, only AWS and tools costs counted\nItem Cost AWS hosting (6 months dev/staging @ $5/mo) $30 GitHub Pro (team of 5) $0 ↳ (Can use free tier) Domain name (.com) $12/year Third-party libraries (optional) $0 TOTAL DEVELOPMENT COST ~$42 Note: Personnel costs NOT included as this is an in-house team with fixed salaries\nAnnual Operating Costs (post go-live) Item Cost/Year AWS Hosting (Phase 1: 100 users) $100-150 Third-party services (SMS for MFA - optional) $100 Domain renewal $12 TOTAL OPERATING/YEAR (Phase 1) ~$212-262 ROI Analysis (In-house project) Initial Investment:\nSetup + Dev tools: ~$42 AWS (6 months dev): ~$30 Total initial: ~$72 First Year Operating Costs:\nPhase 1 (6 months, 100 users): $60 Phase 2 (6 months, 200 users): $150 Total Year 1: ~$210 Total Year 1 Cost: ~$282\nSavings vs Alternatives:\nSAP SuccessFactors: $8-15/user/month = $9,600-18,000/year BambooHR: $6-10/user/month = $7,200-12,000/year Manual Excel: 1 FTE HR admin = $12,000/year Year 1 Savings: $6,918 - $17,718\nYear 1 ROI: 2,454% - 6,281% 🚀\n7. Risk Assessment \u0026amp; Mitigation Risk Impact Probability Mitigation DynamoDB costs spike Medium Low On-demand billing, CloudWatch alarms at $30 threshold Lambda cold starts Low Medium Keep functions warm, optimize bundle size \u0026lt;1MB API Gateway rate limits Medium Low Default 10K req/s sufficient, implement caching Vendor lock-in (AWS) Medium High Use Serverless Framework for portability Team learning curve Low Medium Start with 1-2 Lambda functions, expand gradually Cost Optimization Best Practices Lambda: Bundle size \u0026lt;1MB, reuse connections, avoid cold starts. DynamoDB: Single-table design, use GSIs carefully, on-demand billing. S3: Lifecycle policies to Glacier, presigned URLs, CloudFront caching. API Gateway: Response caching (30-60s), throttling. CloudWatch: Log retention 7 days, filter unnecessary logs. 8. Expected Outcomes Technical Improvements 85% HR processes automated. Real-time dashboard with data \u0026lt; 5 seconds old. \u0026lt; 1s API response time (P95) with Lambda. 70% employees use mobile app. Zero server maintenance. Infinite scalability with serverless. Business Value HR team reduces 60% manual workload. Employee satisfaction increases 40% (self-service). 100% audit trail for compliance. Payroll accuracy 99.5%. Cost savings $6,900-17,700/year vs alternatives. Operating cost only $8-12/month for 100 users. Long-term Vision Scale to 500 users at ~$88/month cost. Integrate AI/ML (AWS Bedrock) for predictive analytics. Multi-branch operations. Potential SaaS product. 9. Conclusion HR Management System with Serverless Architecture provides:\n✅ Ultra-low cost: Only $8-12/month for 100 users Phase 1\n✅ No upfront cost: ~$72 setup, no outsourcing costs\n✅ Massive ROI: Save $6,900-17,700/year vs alternatives\n✅ Scalable: Pay-as-you-go, auto-scale to 500+ users\n✅ Zero maintenance: Serverless = no server management\n✅ Fast development: 6 months MVP → production\nThis is an ideal solution for startups/SMEs with in-house teams wanting to build a modern HR system without large investments.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Implement Face Recognition system backend with Python Flask Develop face detection and recognition algorithms Create user registration with face photo capture Build real-time face recognition attendance system Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Set up face recognition backend API - Implement face detection using OpenCV - Create face encoding generation system 09/08/2025 09/08/2025 2 - Build user registration endpoint with face photos - Implement multi-angle photo capture - Create face encoding storage system 09/09/2025 09/09/2025 3 - Develop face recognition matching algorithm - Implement confidence scoring system - Create face recognition API endpoint 09/10/2025 09/10/2025 4 - Build real-time face recognition frontend component - Integrate camera access and video stream - Implement live face detection display 09/11/2025 09/11/2025 5 - Connect face recognition to attendance system - Implement automatic check-in/out recording - Create attendance logging mechanism 09/12/2025 09/12/2025 6 - Test face recognition accuracy and performance - Optimize recognition speed - Handle edge cases and error scenarios 09/13/2025 09/13/2025 Week 10 Achievements: Successfully implemented comprehensive Face Recognition Attendance system:\nDeveloped robust face recognition backend:\nFlask API endpoints for face recognition operations Face detection using OpenCV and dlib libraries Face encoding generation with face_recognition library Model training system for improved accuracy Face encoding storage and retrieval system Built user registration with face recognition:\nMulti-photo capture from different angles Face encoding generation during registration Storage of face encodings in database User ID and name association with face data Registration validation and error handling Implemented real-time face recognition:\nLive camera feed integration Real-time face detection and tracking Face matching against registered users Confidence score calculation and display Recognition result visualization Created attendance recording system:\nAutomatic check-in/out detection Attendance logging with timestamp User identification and verification Attendance history tracking Duplicate entry prevention Developed frontend face recognition interface:\nCamera access and video stream component Live face detection overlay Recognition status indicators Confidence score display User information display upon recognition Optimized system performance:\nEfficient face encoding comparison algorithms Fast face detection processing Optimized database queries for face matching Reduced recognition latency Memory-efficient face encoding storage Implemented error handling and edge cases:\nNo face detected scenarios Multiple faces in frame handling Low confidence recognition handling Camera access errors Network connectivity issues Created comprehensive testing:\nAccuracy testing with various lighting conditions Performance benchmarking Edge case validation User experience testing System reliability verification "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Integrate Attendance Tracking with face recognition system Develop Payroll Management module with calculation engine Build Task Management system for employee assignments Implement Internal Chat communication feature Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Build Attendance Tracking dashboard - Create attendance history view - Implement attendance statistics and analytics 09/15/2025 09/15/2025 2 - Develop Payroll Management module - Create salary calculation engine - Implement payroll policies and rules 09/16/2025 09/16/2025 3 - Build Task Management system - Create task assignment interface - Implement task tracking and status updates 09/17/2025 09/17/2025 4 - Develop Internal Chat feature - Implement real-time messaging - Create chat interface and notifications 09/18/2025 09/18/2025 5 - Integrate all modules with existing system - Test cross-module functionality - Fix integration issues 09/19/2025 09/19/2025 6 - Optimize system performance - Add data validation and error handling - Improve user experience across modules 09/20/2025 09/20/2025 Week 11 Achievements: Successfully implemented advanced HR management features:\nCompleted Attendance Tracking integration:\nAttendance dashboard with real-time statistics Daily, weekly, and monthly attendance views Attendance history with search and filter Integration with face recognition check-in/out Attendance reports generation Late arrival and early departure tracking Attendance analytics and insights Developed comprehensive Payroll Management:\nSalary calculation engine with configurable rules Payroll policy management (overtime, bonuses, deductions) Employee salary structure management Payroll generation and processing Payroll history and records Salary slip generation and download Tax calculation and compliance Built Task Management system:\nTask creation and assignment interface Task status tracking (Pending, In Progress, Completed) Task priority and deadline management Task assignment to employees or teams Task progress tracking and updates Task comments and attachments Task filtering and search functionality Implemented Internal Chat system:\nReal-time messaging between employees One-on-one and group chat support Message notifications and alerts Chat history and search File sharing in chat Online/offline status indicators Message read receipts Integrated all modules seamlessly:\nCross-module data synchronization Unified user experience across features Consistent API communication patterns Shared component library utilization Integrated navigation and routing Enhanced system performance:\nOptimized database queries Implemented caching strategies Reduced API call overhead Improved page load times Optimized real-time updates Added comprehensive validation:\nForm validation across all modules Data integrity checks Business rule validation Error handling and user feedback Input sanitization and security Improved user experience:\nConsistent UI/UX patterns Intuitive navigation Helpful error messages Loading states and progress indicators Responsive design for all features "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Develop Reports \u0026amp; Analytics module with data visualization Implement Document Management system Conduct comprehensive system testing Prepare project for deployment and finalize documentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 1 - Build Reports \u0026amp; Analytics dashboard - Implement data visualization with charts - Create attendance and payroll reports 09/22/2025 09/22/2025 2 - Develop Document Management system - Implement file upload and storage - Create document categorization and search 09/23/2025 09/23/2025 3 - Conduct comprehensive system testing - Perform unit and integration testing - Test all features and edge cases 09/24/2025 09/24/2025 4 - Fix bugs and optimize performance - Improve error handling - Enhance security measures 09/25/2025 09/25/2025 5 - Prepare deployment configuration - Set up production environment - Configure Docker and deployment scripts 09/26/2025 09/26/2025 6 - Finalize project documentation - Create user manual and API documentation - Prepare project presentation 09/27/2025 09/27/2025 Week 12 Achievements: Successfully completed HR Management System project with all features:\nDeveloped comprehensive Reports \u0026amp; Analytics:\nInteractive dashboard with key performance indicators Attendance reports with multiple filter options Payroll reports with detailed breakdowns Employee performance analytics Data visualization with charts and graphs Export functionality (PDF, Excel, CSV) Custom report generation Real-time data updates Implemented Document Management system:\nFile upload with drag-and-drop interface Document categorization and tagging Advanced search and filter capabilities Document version control Secure file storage and access control Document preview functionality Bulk document operations Document sharing and permissions Conducted thorough system testing:\nUnit testing for individual components Integration testing for module interactions End-to-end testing for complete workflows Performance testing and optimization Security testing and vulnerability assessment User acceptance testing Cross-browser compatibility testing Mobile responsiveness validation Fixed all identified issues:\nResolved bugs and error handling Optimized database queries Improved application performance Enhanced security measures Fixed UI/UX inconsistencies Resolved integration issues Optimized face recognition accuracy Prepared deployment configuration:\nDocker containerization for frontend and backend Environment variable configuration Production build optimization Deployment scripts and automation Database migration scripts Backup and recovery procedures Monitoring and logging setup Created comprehensive documentation:\nComplete project README with setup instructions API documentation with endpoint details User manual with feature guides Developer documentation for codebase Deployment guide for production setup Architecture and design documentation Troubleshooting and FAQ section Finalized project deliverables:\nFully functional HR Management System Face Recognition Attendance system Complete source code with comments Comprehensive test suite Deployment-ready application Project presentation materials Demo video and screenshots Achieved project goals:\nAll core features implemented and tested Modern, responsive user interface Secure and scalable architecture High-performance face recognition system Production-ready application Well-documented codebase Successful project completion "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1\u0026rdquo; Event Objectives The event aimed to explore Generative AI (Gen AI) and its applications, focusing on foundation models, Amazon Bedrock, and how cloud technology can help organizations modernize their platforms while reducing costs compared to building new infrastructure from scratch.\nSpeakers Lâm Tuấn Kiệt - Senior DevOps Engineer, FPT Software\nĐặng Hoàng Hiếu Nghĩa - AI Engineer, Renova Cloud\nĐinh Lê Hoàng Anh - Cloud Engineer Trainee, First Cloud AI Journey\nKey Highlights The Challenge with Legacy Platforms Current platforms are outdated and require modernization\nBuilding new platforms from scratch requires high investment costs\nSolution: Leverage cloud technology and server infrastructure to reduce costs while modernizing\nUnderstanding Generative AI (Gen AI) Definition:\nGen AI is the production of text or images based on input prompts\nIt is a foundation model that can handle multiple tasks and is more generalized than traditional AI models\nKey Characteristics:\nFoundation models are versatile and can be applied to various use cases\nMore generalized approach compared to task-specific AI models\nCan generate content (text, images) based on prompts\nAmazon Bedrock Overview:\nAmazon Bedrock is a fully managed service that makes foundation models from leading AI companies accessible via an API\nIt retrieves internal information along with user prompts to generate the most polished output\nKey Features:\nAccess to multiple foundation models through a single API\nRetrieval of internal/private information\nIntegration with user prompts\nGeneration of high-quality, well-formatted outputs\nKey Takeaways Cloud Migration Strategy:\nInstead of investing heavily in new platform infrastructure, organizations can leverage cloud services\nCloud technology offers cost-effective solutions for platform modernization\nServer-based cloud infrastructure provides scalability and flexibility\nGenerative AI Fundamentals:\nGen AI produces content (text/images) based on input prompts\nFoundation models are more versatile than traditional task-specific models\nThese models can handle multiple tasks with a generalized approach\nAmazon Bedrock Benefits:\nProvides access to multiple foundation models through unified API\nCan retrieve and utilize internal/private information\nGenerates polished, professional outputs by combining internal data with user prompts\nFully managed service reduces operational overhead\nCost Optimization:\nCloud migration can significantly reduce infrastructure costs\nNo need for large upfront investment in new platform development\nPay-as-you-go model provides financial flexibility\nApplying to Work Evaluate Cloud Migration Opportunities:\nAssess current legacy platforms and identify modernization needs\nCompare costs of building new infrastructure vs. cloud migration\nDevelop migration strategies leveraging cloud services\nExplore Gen AI Applications:\nIdentify use cases where Gen AI can add value (content generation, automation)\nUnderstand foundation models and their capabilities\nExperiment with prompt engineering for better outputs\nLeverage Amazon Bedrock:\nEvaluate Amazon Bedrock for accessing foundation models\nIntegrate internal data sources with Bedrock for enhanced outputs\nDevelop workflows that combine internal information with AI-generated content\nCost-Benefit Analysis:\nConduct thorough cost analysis before platform modernization decisions\nConsider cloud solutions as alternatives to building from scratch\nFactor in scalability and operational costs in decision-making\nEvent Experience The event provided valuable insights into the intersection of cloud technology and Generative AI. The speakers shared practical experiences from their respective roles, offering different perspectives on how organizations can leverage modern technologies.\nLearning from Industry Experts DevOps Perspective: Understanding infrastructure challenges and cloud migration strategies from a senior DevOps engineer\nAI Engineering View: Deep dive into Gen AI and foundation models from an AI engineer\u0026rsquo;s perspective\nTrainee Insights: Fresh perspective on cloud engineering and learning journey in the AI space\nTechnical Understanding Gained clarity on what Gen AI is and how it differs from traditional AI approaches\nUnderstood the concept of foundation models and their generalized capabilities\nLearned about Amazon Bedrock as a managed service for accessing AI models\nRecognized the importance of combining internal data with AI prompts for better outputs\nBusiness Value Realized that cloud migration can be a cost-effective alternative to building new platforms\nUnderstood how Gen AI can be applied to generate content and automate tasks\nLearned about the practical applications of Amazon Bedrock in enterprise settings\nSome event photos Overall, the event successfully bridged the gap between cloud infrastructure and Generative AI, showing how organizations can modernize their platforms cost-effectively while leveraging cutting-edge AI capabilities through services like Amazon Bedrock.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.3-s3-vpc/","title":"Create DynamoDB Table","tags":[],"description":"","content":"Create DynamoDB Table DynamoDB is a NoSQL database that will store our tasks. Let\u0026rsquo;s create a table to store task data.\nStep 1: Navigate to DynamoDB Go to AWS Console Search for \u0026ldquo;DynamoDB\u0026rdquo; in the services search bar Click on DynamoDB Step 2: Create Table Click Create table button Configure the table: Table name: tasks Partition key: id (type: String) Table settings: Use default settings Capacity mode: On-demand (recommended for this workshop) Step 3: Create Table Scroll down and click Create table Wait for the table to be created (usually takes a few seconds) Table Structure Our DynamoDB table will have the following structure:\nid (String) - Primary key - Unique identifier for each task title (String) - Task title description (String) - Task description status (String) - Task status (e.g., \u0026ldquo;pending\u0026rdquo;, \u0026ldquo;completed\u0026rdquo;) createdAt (String) - Timestamp when task was created updatedAt (String) - Timestamp when task was last updated Next Steps Once the table is created, we\u0026rsquo;ll proceed to create Lambda functions that will interact with this table.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section lists and introduces the blogs you have translated:\nBlog 1 - Orchestrating document processing with AWS AppSync Events and Amazon Bedrock This blog introduces a solution that simplifies the creation of intelligent document processing workflows, with a web application that allows customers to upload files (documents and images) and gather insights from them (summarization, field extraction, and classification). The solution primarily uses serverless technology, including a websocket to receive real-time insights. The article covers Amazon Bedrock Data Automation, AWS Step Functions, AWS Lambda, AWS AppSync Events, AWS Amplify, Amazon EventBridge, Amazon Cognito, and Amazon S3.\nBlog 2 - How federal agency leaders are using AWS tools to improve efficiency This blog highlights how federal agency leaders are using Amazon Web Services (AWS) to improve efficiency. The article features case studies from the Federal Communications Commission (FCC), Kansas City National Security Campus (KCNSC), National Ignition Facility (NIF), and United States Patent and Trademark Office (USPTO), showcasing how these agencies modernize systems, increase productivity, and improve their ability to serve the public effectively using cloud tools and AI technologies.\nBlog 3 - How federal agency leaders are using AWS tools to improve efficiency This blog highlights how federal agency leaders are using Amazon Web Services (AWS) to improve efficiency. The article features case studies from the Federal Communications Commission (FCC), Kansas City National Security Campus (KCNSC), National Ignition Facility (NIF), and United States Patent and Trademark Office (USPTO), showcasing how these agencies modernize systems, increase productivity, and improve their ability to serve the public effectively using cloud tools and AI technologies.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2\u0026rdquo; Event Objectives The event aimed to explore DevOps mindset and best practices, focusing on understanding what DevOps is, its core principles, measurement strategies, and practical approaches to implementing DevOps culture in organizations.\nSpeakers Trương Quang Tịnh - AWS Community Builder, Platform Engineer at tymeX\nKey Highlights What is DevOps? Definition:\nDevOps is a person who understands and bridges the gap between Development (Dev) and Operations (Ops)\nThere are various tools for managing applications\nEverything must be automated because code reduces work-related risks\nMust have systems for evaluation and monitoring to have sources for assessing how applications run\nCore DevOps Principles 1. Collaboration \u0026amp; Shared Responsibility\nContent:\nDevOps emphasizes that the development (Dev) and operations (Ops) teams must share responsibility for the entire product lifecycle\nTypical quote: \u0026ldquo;You build it, you run it\u0026rdquo;\nWhy it matters:\nReduces conflict between Dev and Ops\nIncreases initiative in fixing bugs and improving features\nReduces deployment and troubleshooting time\nReal-life example:\nA Dev team deploys a new microservice on AWS Lambda. When there is a bug that increases API latency, the Dev team analyzes the logs from CloudWatch and fixes it without \u0026ldquo;handing the ball\u0026rdquo; to Ops\nIn companies like Amazon, the development team is responsible 24/7 for the service they create\n2. Automation Everything\nContent:\nDevOps prioritizes automation as much as possible: CI/CD, test, provisioning, monitoring, rollback, security scanning\u0026hellip;\nKey principle: \u0026ldquo;Manual work is technical debt\u0026rdquo;\nImportant reasons:\nReduce human error\nAccelerate deployment\nMake it easy to repeat the process\nEnable rapid scaling\nReal-life examples:\nUse GitHub Actions + Terraform to automatically create AWS infrastructure each time the pipeline runs\nEach Pull Request automatically runs unit tests, linting, and security scans using tools like SonarQube, Snyk\nDeploy production applications using ArgoCD or Jenkins with just 1 commit\n3. Continuous Learning \u0026amp; Experimentation\nContent:\nDevOps culture encourages learning new things, experimenting, and accepting small failures\nKey principles: \u0026ldquo;Fail fast, learn faster\u0026rdquo; and \u0026ldquo;Experiment with new tools and practices\u0026rdquo;\nWhy it matters:\nTechnology changes very quickly—DevOps must stay up to date\nExperimentation helps improve continuous improvement\nReduce fear of failure → increase innovation\nReal-life examples:\nA team switched from Jenkins to GitHub Actions to reduce pipeline runtime from 15 minutes to 5 minutes\nHost game days on AWS to practice crashing simulated services and test resilience\nExperiment with Canary Deployment or Blue-Green Deployment to reduce risk when deploying new features\n4. Measurement\nContent:\nDevOps requires continuous and real-time measurement\nExamples: logs, metrics, tracing, SLOs, SLIs\nWhy it\u0026rsquo;s important:\nCan\u0026rsquo;t improve what can\u0026rsquo;t be measured\nUnderstand the system health and behavior of the product\nReduce time to detect failure (MTTD) and time to recover (MTTR)\nPractical examples:\nUse Prometheus + Grafana to monitor CPU, latency, error rate\nSet up AWS CloudWatch Alarms to alert when 5xx error rate exceeds 2%\nMonitor DORA metrics: Deployment Frequency, Lead Time for Changes, MTTR, Change Failure Rate\nDevOps Goals and Benefits Monitor Deployment Health:\nMonitor the status of deployments: are there any errors, are there rollbacks, how long does it take to deploy\nHelps detect deployment errors early\nReduces production risks\nExample: Use AWS CodeDeploy or ArgoCD to track the status of each deployment and automatically rollback if the error rate \u0026gt; 5%\nImprove Agility:\nAgility = the ability to change quickly\nDevOps helps businesses release more, faster, and more confidently\nRelease features earlier than competitors\nShorten the time from idea to product\nExample: A startup deploys 20–30 times/day thanks to CI/CD + microservices\nEnsure System Stability:\nStability = the system runs stably, with little downtime\nAvoid losing revenue\nIncrease trust with customers\nExample: Use AWS autoscaling + health checks to ensure the service runs smoothly even when traffic spikes\nOptimize Customer Experience:\nObserving metrics to improve: response speed, error rate, downtime → better experience\nExample: Reduce API latency from 500ms to 120ms by optimizing infrastructure and pipeline\nJustify Technology Investments:\nUse data to prove: DevOps helps reduce errors, speed up releases, and save costs\nExample: Use DORA metrics to report to leadership that implementing CI/CD reduces time to market by 40%\nKey Takeaways DevOps is a Mindset, Not Just Tools:\nDevOps is about bridging Dev and Ops, not just using specific tools\nUnderstanding the system is more important than knowing tools\nAutomation reduces risks and human errors\nFour Core Principles:\nCollaboration \u0026amp; Shared Responsibility: \u0026ldquo;You build it, you run it\u0026rdquo;\nAutomation Everything: Manual work is technical debt\nContinuous Learning \u0026amp; Experimentation: Fail fast, learn faster\nMeasurement: Can\u0026rsquo;t improve what can\u0026rsquo;t be measured\nMeasurement is Critical:\nUse logs, metrics, tracing, SLOs, SLIs\nMonitor DORA metrics for continuous improvement\nReal-time monitoring helps reduce MTTD and MTTR\nDevOps Benefits:\nImprove agility and deployment frequency\nEnsure system stability and reduce downtime\nOptimize customer experience\nJustify technology investments with data\nBest Practices:\nStart with fundamentals (Linux, networking, Git, cloud basics)\nLearn by building real projects\nDocument everything for better collaboration\nApplying to Work Implement Shared Responsibility:\nEncourage Dev teams to take ownership of their services\nEstablish 24/7 responsibility for services created\nReduce handoffs between Dev and Ops teams\nAutomate Everything:\nSet up CI/CD pipelines using GitHub Actions or Jenkins\nAutomate infrastructure provisioning with Terraform\nImplement automated testing, linting, and security scanning\nUse tools like ArgoCD for automated deployments\nEstablish Measurement Systems:\nSet up monitoring with Prometheus + Grafana or AWS CloudWatch\nTrack DORA metrics (Deployment Frequency, Lead Time, MTTR, Change Failure Rate)\nConfigure alerts for error rates and system health\nMonitor deployment health and implement automatic rollbacks\nFoster Continuous Learning:\nOrganize game days to practice resilience testing\nExperiment with deployment strategies (Canary, Blue-Green)\nStay updated with new tools and practices\nEncourage experimentation and learning from failures\nStart with Fundamentals:\nBuild strong foundation in Linux, networking, Git, and cloud basics\nLearn by building real projects (Docker + Nginx, CI/CD pipelines, monitoring)\nDocument everything: steps, errors, and solutions\nEvent Experience The event provided comprehensive insights into DevOps mindset and culture, going beyond just tools and focusing on the fundamental principles that drive successful DevOps implementation.\nUnderstanding DevOps Culture Learned that DevOps is fundamentally about bridging Dev and Ops, not just using specific tools\nUnderstood the importance of shared responsibility and collaboration\nRecognized that automation is essential to reduce risks and human errors\nCore Principles in Practice Collaboration: Realized how \u0026ldquo;You build it, you run it\u0026rdquo; changes team dynamics and accountability\nAutomation: Understood why \u0026ldquo;Manual work is technical debt\u0026rdquo; and how automation accelerates everything\nLearning: Appreciated the value of \u0026ldquo;Fail fast, learn faster\u0026rdquo; mindset in fostering innovation\nMeasurement: Recognized that measurement is the foundation for continuous improvement\nPractical Applications Learned about real-world examples from companies like Amazon\nUnderstood how to implement monitoring and measurement systems\nGained insights into deployment strategies and automation tools\nDiscovered the importance of DORA metrics for tracking DevOps success\nKey Insights DevOps is a cultural shift, not just a set of tools\nMeasurement and monitoring are critical for success\nAutomation reduces risks and enables rapid scaling\nContinuous learning and experimentation drive innovation\nSome event photos Overall, the event successfully conveyed that DevOps is fundamentally about culture, collaboration, and continuous improvement. The emphasis on measurement, automation, and shared responsibility provides a solid foundation for implementing DevOps practices that deliver real business value.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/","title":"Create Lambda Functions","tags":[],"description":"","content":"Create Lambda Functions We\u0026rsquo;ll create Lambda functions to handle CRUD operations for our tasks API.\nStep 1: Create IAM Role for Lambda First, we need to create an IAM role that allows Lambda to access DynamoDB.\nGo to IAM service in AWS Console Click Roles → Create role Select AWS service → Lambda Click Next Attach policy: AmazonDynamoDBFullAccess (for this workshop) Role name: lambda-dynamodb-role Click Create role Step 2: Create Lambda Function - Create Task Go to Lambda service Click Create function Choose Author from scratch Function name: createTask Runtime: Python 3.11 Execution role: Use existing role → Select lambda-dynamodb-role Click Create function Lambda Function Code - Create Task Replace the default code with:\nimport json import boto3 import uuid from datetime import datetime dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: body = json.loads(event[\u0026#39;body\u0026#39;]) task = { \u0026#39;id\u0026#39;: str(uuid.uuid4()), \u0026#39;title\u0026#39;: body[\u0026#39;title\u0026#39;], \u0026#39;description\u0026#39;: body.get(\u0026#39;description\u0026#39;, \u0026#39;\u0026#39;), \u0026#39;status\u0026#39;: body.get(\u0026#39;status\u0026#39;, \u0026#39;pending\u0026#39;), \u0026#39;createdAt\u0026#39;: datetime.utcnow().isoformat(), \u0026#39;updatedAt\u0026#39;: datetime.utcnow().isoformat() } table.put_item(Item=task) return { \u0026#39;statusCode\u0026#39;: 201, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(task) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 3: Create Lambda Function - List Tasks Create another function: listTasks Same configuration as above Use this code: import json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: response = table.scan() tasks = response[\u0026#39;Items\u0026#39;] return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(tasks) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 4: Create Lambda Function - Get Task Function name: getTask\nimport json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: task_id = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) if \u0026#39;Item\u0026#39; not in response: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Task not found\u0026#39;}) } return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(response[\u0026#39;Item\u0026#39;]) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 5: Create Lambda Function - Update Task Function name: updateTask\nimport json import boto3 from datetime import datetime dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: task_id = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] body = json.loads(event[\u0026#39;body\u0026#39;]) # Get existing task response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) if \u0026#39;Item\u0026#39; not in response: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Task not found\u0026#39;}) } # Update task update_expression = \u0026#34;SET updatedAt = :updatedAt\u0026#34; expression_values = { \u0026#39;:updatedAt\u0026#39;: datetime.utcnow().isoformat() } if \u0026#39;title\u0026#39; in body: update_expression += \u0026#34;, title = :title\u0026#34; expression_values[\u0026#39;:title\u0026#39;] = body[\u0026#39;title\u0026#39;] if \u0026#39;description\u0026#39; in body: update_expression += \u0026#34;, description = :description\u0026#34; expression_values[\u0026#39;:description\u0026#39;] = body[\u0026#39;description\u0026#39;] if \u0026#39;status\u0026#39; in body: update_expression += \u0026#34;, #status = :status\u0026#34; expression_values[\u0026#39;:status\u0026#39;] = body[\u0026#39;status\u0026#39;] table.update_item( Key={\u0026#39;id\u0026#39;: task_id}, UpdateExpression=update_expression, ExpressionAttributeValues=expression_values, ExpressionAttributeNames={\u0026#39;#status\u0026#39;: \u0026#39;status\u0026#39;}, ReturnValues=\u0026#39;ALL_NEW\u0026#39; ) # Get updated task updated_response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps(updated_response[\u0026#39;Item\u0026#39;]) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Step 6: Create Lambda Function - Delete Task Function name: deleteTask\nimport json import boto3 dynamodb = boto3.resource(\u0026#39;dynamodb\u0026#39;) table = dynamodb.Table(\u0026#39;tasks\u0026#39;) def lambda_handler(event, context): try: task_id = event[\u0026#39;pathParameters\u0026#39;][\u0026#39;id\u0026#39;] # Check if task exists response = table.get_item(Key={\u0026#39;id\u0026#39;: task_id}) if \u0026#39;Item\u0026#39; not in response: return { \u0026#39;statusCode\u0026#39;: 404, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: \u0026#39;Task not found\u0026#39;}) } # Delete task table.delete_item(Key={\u0026#39;id\u0026#39;: task_id}) return { \u0026#39;statusCode\u0026#39;: 200, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;message\u0026#39;: \u0026#39;Task deleted successfully\u0026#39;}) } except Exception as e: return { \u0026#39;statusCode\u0026#39;: 500, \u0026#39;headers\u0026#39;: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Access-Control-Allow-Origin\u0026#39;: \u0026#39;*\u0026#39; }, \u0026#39;body\u0026#39;: json.dumps({\u0026#39;error\u0026#39;: str(e)}) } Next Steps Now that we have all Lambda functions created, we\u0026rsquo;ll configure API Gateway to connect them together.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":" In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendence, event support, speaker, etc.) A brief description of the event\u0026rsquo;s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: GenAI-powered App-DB Modernization workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 2 Event Name: AWS GenAI Builders Club\nDate \u0026amp; Time: October 3, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 3 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 4 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\nEvent 5 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendence\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/4-eventparticipated/4.5-event5/","title":"Event 5","tags":[],"description":"","content":"Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3\u0026rdquo; Event Objectives The event aimed to explore AWS Identity and Access Management (IAM) best practices, focusing on security principles, role-based access control, Single Sign-On (SSO), AWS Organizations, credential management, and advanced IAM features for enterprise security.\nSpeakers Huỳnh Hoàng Long\nĐinh Lê Hoàng Anh\nKey Highlights Understanding IAM (Identity and Access Management) Definition:\nIAM consists of roles with specific permissions assigned to specific users\nIncreases authentication capabilities for the system\nProvides fine-grained access control to AWS resources\nCore Concept:\nRoles define what actions can be performed\nUsers are assigned roles based on their responsibilities\nEnhances system security through proper access management\nIAM Best Practices 1. Principle of Least Privilege:\nBest practice: Do not grant an account too many permissions\nOnly grant specific permissions needed for the task\nReduces security risks and potential damage from compromised accounts\n2. Root Access Key Management:\nDelete root access keys because they have the highest privileges\nIf access keys are lost, it won\u0026rsquo;t cause too much impact\nMinimizes maximum potential losses\nRoot account should only be used for initial setup and emergency scenarios\n3. Avoid Wildcard Permissions:\nAvoid using * (asterisk) which means full permissions for all services\nUse specific service and action permissions instead\nProvides better security and auditability\n4. Use AWS Login (Temporary Credentials):\nShould use AWS login because it resets at least every 15 minutes and at most every 36 hours\nIncreases security significantly\nTemporary credentials reduce the risk of long-term exposure\nAutomatically expires, reducing attack surface\n5. Multi-Factor Authentication (MFA):\nMFA is an additional authentication step that helps prevent unauthorized access\nPrevents account login from other locations\nAdds an extra layer of security beyond passwords\nShould be enabled for all privileged accounts\n6. Regular Password Changes:\nShould regularly change account passwords\nFollows security best practices for credential management\nReduces risk of compromised credentials\nSingle Sign-On (SSO) Definition:\nSSO allows one login to access multiple systems\nUsers authenticate once and gain access to multiple applications\nWhen switching to another app, no need to login again\nEnterprise Benefits:\nBetter management in enterprise environments\nMultiple roles available, each with specific accounts\nCentralized access control\nSimplified user experience\nAWS Integration:\nWhen activating SSO, simultaneously activates AWS Organizations\nEnables centralized management of multiple AWS accounts\nProvides unified access control across the organization\nAWS Organizations and Service Control Policies (SCP) AWS Organizations:\nEnables management of multiple AWS accounts from a central location\nProvides consolidated billing and account management\nWorks in conjunction with SSO for enterprise access control\nService Control Policies (SCP):\nService control policies that can set maximum permission limits within an account\nProvides centralized security governance\nCan restrict what actions can be performed across accounts\nHelps enforce organizational security policies\nPermission Boundaries Definition:\nAn advanced IAM service that handles centralized security issues\nSets the maximum permissions that an identity-based policy can grant\nProvides an additional layer of security control\nPrevents privilege escalation even if policies are misconfigured\nUse Cases:\nDelegating permissions to developers or teams\nEnsuring users cannot exceed intended permissions\nCentralized security management in large organizations\nCredential Spectrum and Rotation Credential Spectrum:\nUnderstanding different types of credentials and their security implications\nFrom long-term access keys to temporary credentials\nBalancing security with usability\nCredential Rotation:\nAbout AWS Secrets Manager service\nProcess: Create fake accounts with new passwords, test if they can be used, apply to your account, and finally delete old accounts\nAutomates the rotation of database credentials, API keys, and other secrets\nReduces risk of credential compromise\nEnsures credentials are regularly updated without manual intervention\nAWS Secrets Manager Benefits:\nAutomatic rotation of secrets\nSecure storage of credentials\nIntegration with AWS services\nAudit trail of secret access\nIAM Access Analyzer Overview:\nTool for analyzing IAM policies and access patterns\nIdentifies resources shared with external entities\nHelps discover unintended access\nProvides recommendations for improving security posture\nKey Features:\nAnalyzes resource-based policies\nIdentifies publicly accessible resources\nSuggests policy improvements\nContinuous monitoring of access patterns\nPolicy Analysis:\nDetects policies containing Principal: * (public access)\nFlags security issues even when conditions are present\nHelps identify and remediate security risks\nAutomated Remediation:\nCan integrate with EventBridge for automated responses\nLambda functions can add deny statements to IAM roles\nSNS notifications for security team alerts\nEnables proactive security management\nKey Takeaways IAM Fundamentals:\nIAM roles and users provide granular access control\nProper IAM configuration is essential for AWS security\nAuthentication and authorization are separate but related concepts\nSecurity Best Practices:\nFollow principle of least privilege\nDelete root access keys\nAvoid wildcard permissions\nUse temporary credentials (AWS login)\nEnable MFA for all accounts\nRegularly rotate passwords\nEnterprise Access Management:\nSSO simplifies access across multiple systems\nAWS Organizations enables centralized account management\nSCPs provide organization-wide security governance\nPermission boundaries prevent privilege escalation\nCredential Management:\nUse AWS Secrets Manager for credential rotation\nPrefer temporary credentials over long-term access keys\nAutomate credential rotation where possible\nMonitor credential usage and access patterns\nContinuous Security:\nUse IAM Access Analyzer for ongoing security assessment\nRegularly review and audit IAM policies\nMonitor for unintended access\nStay updated with AWS security best practices\nApplying to Work Implement IAM Best Practices:\nReview and remove root access keys\nEnable MFA for all privileged accounts\nReplace wildcard permissions with specific permissions\nUse temporary credentials instead of long-term access keys\nSet Up SSO:\nEvaluate SSO implementation for your organization\nConfigure AWS Organizations if managing multiple accounts\nImplement role-based access control\nCentralize user management\nImplement Permission Boundaries:\nUse permission boundaries for delegated access\nSet maximum permission limits for teams\nPrevent privilege escalation\nCentralize security policies\nCredential Rotation:\nUse AWS Secrets Manager for database credentials\nAutomate rotation of API keys and secrets\nEstablish rotation schedules\nTest rotation processes before production\nSecurity Monitoring:\nEnable IAM Access Analyzer\nRegularly review access patterns\nIdentify and remediate unintended access\nUse CloudTrail for audit logging\nRegular Security Audits:\nConduct periodic IAM policy reviews\nRemove unused credentials and roles\nUpdate policies based on changing requirements\nDocument access requirements and justifications\nEvent Experience The event provided comprehensive insights into AWS IAM security best practices, covering everything from basic principles to advanced enterprise features. The speakers shared practical experiences and real-world scenarios that highlighted the importance of proper IAM configuration.\nUnderstanding IAM Fundamentals Learned that IAM is about roles with specific permissions assigned to users\nUnderstood how proper IAM configuration enhances system authentication\nRecognized the importance of granular access control\nSecurity Best Practices Least Privilege: Realized the importance of granting only necessary permissions\nRoot Access: Understood why root access keys should be deleted\nWildcard Avoidance: Learned to avoid * permissions for better security\nTemporary Credentials: Appreciated the security benefits of AWS login with automatic rotation\nMFA: Recognized MFA as essential for preventing unauthorized access\nPassword Management: Understood the need for regular password changes\nEnterprise Features SSO: Learned how Single Sign-On simplifies access across multiple systems\nAWS Organizations: Understood how it works with SSO for centralized management\nSCP: Discovered how Service Control Policies enforce organization-wide security\nPermission Boundaries: Recognized their role in preventing privilege escalation\nCredential Management Credential Rotation: Learned about AWS Secrets Manager and automated rotation\nCredential Spectrum: Understood different types of credentials and their security implications\nBest Practices: Discovered the process of credential rotation and testing\nSecurity Tools IAM Access Analyzer: Learned how to use it for continuous security assessment\nMonitoring: Understood the importance of ongoing access pattern analysis\nRemediation: Discovered how to identify and fix security issues\nSome event photos Overall, the event successfully demonstrated that IAM is the foundation of AWS security. The emphasis on best practices, enterprise features, and continuous security monitoring provides a comprehensive approach to managing access and protecting AWS resources effectively.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.5-policy/","title":"Configure API Gateway","tags":[],"description":"","content":"Configure API Gateway Now we\u0026rsquo;ll create a REST API in API Gateway and connect it to our Lambda functions.\nStep 1: Create REST API Go to API Gateway service Click Create API Choose REST API → Build Choose New API API name: TaskManagementAPI Description: Serverless REST API for Task Management Endpoint Type: Regional Click Create API Step 2: Create Resources In the API, you\u0026rsquo;ll see a root resource / Click Actions → Create Resource Resource name: tasks Resource path: tasks Enable API Gateway CORS Click Create Resource Step 3: Create Methods Create POST Method (Create Task) Select /tasks resource Click Actions → Create Method Select POST → Click checkmark Integration type: Lambda Function Lambda Region: Your region Lambda Function: createTask Enable Use Lambda Proxy Integration Click Save → OK (when prompted to add permissions) Create GET Method (List Tasks) Select /tasks resource Click Actions → Create Method Select GET → Click checkmark Integration type: Lambda Function Lambda Function: listTasks Enable Use Lambda Proxy Integration Click Save → OK Create GET Method (Get Single Task) Select /tasks resource Click Actions → Create Resource Resource name: {id} Resource path: {id} Enable API Gateway CORS Click Create Resource Select /tasks/{id} resource Create GET method → Connect to getTask function Create PUT method → Connect to updateTask function Create DELETE method → Connect to deleteTask function Step 4: Enable CORS Select /tasks resource Click Actions → Enable CORS Accept default settings Click Enable CORS and replace existing CORS headers Step 5: Deploy API Click Actions → Deploy API Deployment stage: New Stage Stage name: dev Stage description: Development stage Click Deploy Step 6: Get API Endpoint After deployment, you\u0026rsquo;ll see the Invoke URL. This is your API endpoint.\nExample: https://abc123xyz.execute-api.us-east-1.amazonaws.com/dev\nAPI Endpoints Your API will have the following endpoints:\nPOST /tasks - Create a new task GET /tasks - List all tasks GET /tasks/{id} - Get a specific task PUT /tasks/{id} - Update a task DELETE /tasks/{id} - Delete a task Testing the API You can test the API using:\nAPI Gateway Console - Use the \u0026ldquo;Test\u0026rdquo; feature Postman - Import the API and test endpoints curl - Use command line Browser - For GET requests Example cURL Commands # Create a task curl -X POST https://your-api-id.execute-api.region.amazonaws.com/dev/tasks \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;Learn AWS\u0026#34;, \u0026#34;description\u0026#34;: \u0026#34;Complete Lambda workshop\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;pending\u0026#34;}\u0026#39; # List all tasks curl https://your-api-id.execute-api.region.amazonaws.com/dev/tasks # Get a specific task curl https://your-api-id.execute-api.region.amazonaws.com/dev/tasks/{task-id} # Update a task curl -X PUT https://your-api-id.execute-api.region.amazonaws.com/dev/tasks/{task-id} \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;title\u0026#34;: \u0026#34;Updated title\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;completed\u0026#34;}\u0026#39; # Delete a task curl -X DELETE https://your-api-id.execute-api.region.amazonaws.com/dev/tasks/{task-id} Next Steps Test your API thoroughly, then proceed to cleanup section to remove resources.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building a Serverless REST API with AWS Lambda and API Gateway Overview In this workshop, you will learn how to build a serverless REST API using AWS Lambda and Amazon API Gateway. This architecture allows you to create scalable, cost-effective APIs without managing servers.\nYou will build a simple Task Management API that allows users to:\nCreate new tasks List all tasks Get a specific task Update a task Delete a task The API will use:\nAWS Lambda for serverless compute functions Amazon API Gateway for REST API endpoints Amazon DynamoDB for data storage AWS IAM for security and permissions Learning Objectives By the end of this workshop, you will be able to:\nCreate Lambda functions using Python Set up API Gateway REST API Integrate Lambda with API Gateway Store and retrieve data from DynamoDB Understand serverless architecture patterns Handle API errors and responses properly Prerequisites AWS Account with appropriate permissions Basic knowledge of Python programming Understanding of REST API concepts AWS CLI configured (optional but recommended) Content Workshop Overview Prerequisites and Setup Create DynamoDB Table Create Lambda Functions Configure API Gateway Testing and Cleanup "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at First Cloud Journey from August 12, 2025 to November 12, 2025, I had the opportunity to apply the knowledge and skills learned at school to a real-world software development environment.\nDuring this internship, I participated in the development of a web-based employee management system for a technology company.\nThis system was designed to help organizations manage employee data, attendance, and departmental information efficiently on the cloud.\nMy position in the project team was Frontend Developer, where I focused on building the user interface, integrating APIs, and ensuring responsive design and usability.\nI consistently strived to complete assigned tasks on time, adhered to the company\u0026rsquo;s workflow standards, and collaborated closely with my teammates to deliver high-quality features and improve overall system performance.\nBelow is my self-assessment based on key criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Applying frontend knowledge, using frameworks effectively, delivering UI/UX components ☐ ✅ ☐ 2 Ability to learn Ability to learn new frontend tools, frameworks, and technologies quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative to improve the interface and fix minor bugs without waiting for instructions ☐ ✅ ☐ 4 Sense of responsibility Completing assigned UI modules and tasks on time with good quality ☐ ✅ ☐ 5 Discipline Following company schedules, coding standards, and version control guidelines ☐ ✅ ☐ 6 Progressive mindset Willingness to receive feedback and continuously enhance frontend performance ☐ ✅ ☐ 7 Communication Discussing requirements and updates clearly with the backend and QA teams ☐ ✅ ☐ 8 Teamwork Collaborating effectively with developers and designers in the project ☐ ✅ ☐ 9 Professional conduct Maintaining respect, cooperation, and a positive attitude in the workplace ☐ ✅ ☐ 10 Problem-solving skills Identifying UI/UX issues, debugging, and optimizing components ☐ ✅ ☐ 11 Contribution to project/team Supporting team goals by improving interface quality and suggesting usability enhancements ☐ ✅ ☐ 12 Overall General performance and development throughout the internship period ☐ ✅ ☐ Strengths Acquired hands-on experience in frontend development using React, Tailwind CSS, and integration with RESTful APIs.\nDeveloped a deeper understanding of UI/UX principles, accessibility, and responsive design.\nStrengthened teamwork, communication, and adaptability through active collaboration with backend developers.\nGained practical experience in cloud-based project environments (AWS) and using tools like GitHub and CodePipeline.\nNeeds Improvement Enhance problem-solving and debugging skills: Continue practicing advanced debugging techniques and performance optimization.\nImprove communication clarity: Deliver more concise and structured updates during daily stand-up meetings.\nStrengthen coding discipline: Follow project conventions more consistently, especially in documentation and version control.\nExplore deeper frontend topics: Learn more about advanced state management, testing, and deployment workflows.\nOverall Reflection Overall, I evaluate my internship performance at a Fair level.\nThrough this internship, I gained valuable real-world experience as a frontend developer, improved my technical and teamwork skills, and developed a clearer understanding of how professional cloud-based systems are built and maintained.\nAlthough there is still room for improvement in communication and code optimization, I believe this internship has laid a strong foundation for my future career in web development.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, I can freely share my personal opinions about my experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\n1. Working Environment The working environment at First Cloud Journey (FCJ) is very dynamic and inspiring.\nThe workspace is professional yet comfortable, allowing me to focus and be creative easily.\nCompany members are friendly, open, and always ready to help when I encounter difficulties.\nI think FCJ could organize more team building activities or internal sharing sessions to further strengthen team bonds.\n2. Support from Mentor / Team Lead My mentor provides very detailed guidance and always explains clearly when I don\u0026rsquo;t understand an issue.\nRather than giving direct answers, the mentor encourages me to explore, analyze, and solve problems independently, helping me develop independent thinking skills.\nAdditionally, I had the opportunity to participate in meetings and project discussions, which gave me a better understanding of real-world workflows and technical decision-making processes in the enterprise.\n3. Relevance of Work to Academic Major The tasks I undertook are highly relevant to the Information Technology major.\nAs a Frontend Developer, I was able to apply knowledge about web programming, interface design, and working with databases.\nFurthermore, I also gained exposure to AWS Cloud services, expanding my understanding of modern technology beyond the scope of academic learning.\n4. Learning and Skill Development Opportunities During the internship, I developed both technical and soft skills.\nTechnically, I significantly improved my ability to use React, Tailwind CSS, and Git, as well as how to integrate APIs effectively.\nAdditionally, I learned professional work skills such as time management, communication in a team environment, and Agile workflow processes.\nThrough review sessions and experience sharing from my mentor, I gained many valuable real-world lessons that helped clarify my career direction.\n5. Company Culture and Team Spirit FCJ\u0026rsquo;s culture emphasizes collaboration, learning, and mutual respect.\nEveryone is always willing to share knowledge and provide support when needed, regardless of job position.\nEven during urgent projects, the team spirit remains very positive and united.\nThis made me feel like I was truly part of the team, even as an intern.\n6. Internship Policies and Benefits The company\u0026rsquo;s internship policies are flexible and clear.\nFCJ supports flexible working hours, provides reasonable internship allowances, and especially allows interns to participate in internal technical training sessions.\nWhat I appreciate most is the opportunity to directly participate in real projects, helping me better understand product development processes in the enterprise.\nAdditional Questions What did you find most satisfying during your internship?\nBeing able to directly participate in developing the interface for a real project and receiving trust from both my mentor and the technical team.\nWhat should the company improve for future internship programs?\nConsider organizing additional onboarding training sessions at the beginning of the internship period to help new interns familiarize themselves more quickly with tools, repositories, and project deployment processes.\nWould you recommend this internship program to others? Why?\nYes. I would definitely recommend FCJ to other students because it is a practical learning environment with dedicated mentors, and it helps interns develop professional skills significantly.\nSuggestions and Expectations FCJ could consider organizing monthly mini hackathons or internal tech talks to encourage creativity and idea exchange among members.\nI hope to have the opportunity to continue collaborating or participate in advanced internship programs at FCJ in the future.\nOverall, this has been a valuable and memorable experience that helped me bridge the gap between academic knowledge and practical application, while significantly developing my thinking and professional skills.\n"},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://mynameisbof.github.io/fcj-workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]